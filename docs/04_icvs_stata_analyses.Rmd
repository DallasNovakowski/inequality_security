---
title: "icvs_first_analyses"
output: 
  html_document:
    number_sections: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr) # pipe
library(purrr) # map function
library(car) #qqPlot
load("C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.RData")

# icvs_joined <-  icvs_joined %>%
#   map(. %>% filter(., !is.na(num_victim_5yr)))
```


# Analysis procedure

Despite the possibility of nonnormal data, the large sample size in this study will yield similar variance estimates to REML estimation.

## estimation procedure

Comparison of fixed portion of the model should use deviance tests based on MLE
Comparison of random components can use REML if fixed portions are identical

- S+B suggest standard errors in t-tests of fixed effects should be based on REML in N≤50 (pg. 94)
Sample size in this case is based on number of level 2 units
“Large” samples indicated by N-q-1≥50, where q indicates number of level 2 predictors (S+B pg. 60)

- But, the problem is that statistical weights require FML

Variance components in MLE are downward biased

## Three-level logistic regression wtf




##KR adjustment
- The KR method estimates different SE’s, but not different random effects, from just REML
- SE’s became progressively larger, producing more conservative significance tests
- When possible, with smaller number of clusters, KR may be a good idea

Issue is that in smaller samples, sampling distribution may not be normal
The KR is an “approximation” method that accounts for small-sample bias 
Can be used in Stata with dfmethod(kroger)
Using this method will also invoke t-distribution
The problem is that it can only be used with REML estimation


## interactions?
If focus is on cross-level interactions, use a 50/20 rule
About 50 groups with about 20 individuals per group

## sample size

 When number of groups is less than 20, fixed parameter estimates and their SE’s become inaccurate 

## SWIID aggregation

Given that the SWIID is presented in a list of 100 different dataframes, the analysis will firstly be conducted on each dataframe, with the results stored separately.

Each of these 100 dataframes will then be subjected to 100 simulations, yielding vectors of 10,000 estimates. The means and distributions of these vectors will yield the coefficient and standard error values, respectively.

"We could now use Rubin’s (1987) rules to calculate the mean and standard error for each fixed effect estimate across these 100 sets of results (these rules are implemented in the package mitools (Lumley 2014) and others), but instead we will use simulation. Using sim from the arm package (Gelman and Su 2015), we generate the distribution of fixed-effects estimates in the results both for each dataframe and across the 100 dataframes."

```{r other text, warning=F, message=FALSE, echo=FALSE}
# expected value of the ML variance estimator is not equal to the true variance σ², although it approaches the true variance at large sample sizes
# https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#twolevelmodeling
# 
# The most common methods for estimating model parameters—both fixed effects and variance components—are maximum likelihood (ML) and restricted maximum likelihood (REML)
# 
# REML is conditional on the fixed effects, so that the part of the data used for estimating variance components is separated from that used for estimating fixed effects. Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. REML is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects. ML should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects (with the same fixed effects model).
# 
# assume that random effects follow a normal distribution with mean 0 and a variance parameter which must be estimated from the data.
# 
# the error terms at Level Two can be assumed to follow a multivariate normal distribution
# 
# Certain software packages will report p-values corresponding to hypothesis tests for parameters of fixed effects; these packages are typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution. 

# 
# 
# ML and REML assume errors a normally distributed
# - differences between the two estimation methods occur in variance estimation: at large n-to IV ratios, immaterial difference
# 
# REML preferable as its variance parameters are unbiasesd
# 
# however, ML better for carrying out deviance tests

# The deviance for a linear mixed model is defined as follows:
# 
# deviance=−2∗log likelihood
# 
# The deviance is an index of model fit: a model with a higher deviance provides a poorer model fit to the data than a model with a lower deviance. 
```

```{r estimation preview1, warning=F, message=FALSE}
# Both packages use Lattice as the backend, but nlme has some nice features like groupedData() and lmList() that are lacking in lme4 (IMO). From a practical perspective, the two most important criteria seem, however, that

# https://lme4.r-forge.r-project.org/book/

#    lme4 extends nlme with other link functions: in nlme, you cannot fit outcomes whose distribution is not gaussian, lme4 can be used to fit mixed-effects logistic regression, for example.
#    in nlme, it is possible to specify the variance-covariance matrix for the random effects (e.g. an AR(1)); it is not possible in lme4.

#Now, lme4 can easily handle very huge number of random effects (hence, number of individuals in a given study) thanks to its C part and the use of sparse matrices. The nlme package has somewhat been superseded by lme4 so I won't expect people spending much time developing add-ons on top of nlme. Personally, when I have a continuous response in my model, I tend to use both packages, but I'm now versed to the lme4 way for fitting GLMM.

# nlme let's you specify variance-covariance structures for the residuals (i.e. spatial or temporal autocorrelation or heteroskedasticity), lme4 doesn't

# https://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models
# lmeg or nlme?
#nlme is the very large tool box, including a TIG welder to make any tools you need.

# brms: Bayesian Regression Models using 'Stan'
# https://cran.r-project.org/web/packages/brms/index.html
```


## Main statistics

pseudo-loglikelihoods: can't be compared

**BIC**: Stands for “Bayesian Information Criteria”
A non-parametric approach to model fit
Basically produces a fit statistic that looks at model fit relative to the number of predictors
Penalizes improvement in model fit if it took a large number of predictors

There’s no baseline to the BIC, so you can’t use it as an overall indicator of model fit
However, you can use it as a relative measure of model
Which of two models is better

Typically, you want the lowest BIC possible

strength of evidence : 0-2 - weak; 2-6 = positive, 6-10 = strong, > 10 = very strong
From Long 1997:112


# Stata Analyses

```{r write stata document (.dta)}
iv_2005 <- dplyr::rename(iv_2005, iw_mcity = individual_weight_for_main_cities, 
                  iw_lastmcity = individual_weight_for_last_main_city, 
         iw_nat = individual_weight_for_national_surveys, 
       lik_elected = likely_elected_minicipal_councilors)

iv_2005_mod <- dplyr::rename(iv_2005_mod, iw_mcity = individual_weight_for_main_cities, 
                  iw_lastmcity = individual_weight_for_last_main_city, 
         iw_nat = individual_weight_for_national_surveys, 
       lik_elected = likely_elected_minicipal_councilors)

haven::write_dta(iv_2005,"C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta")
haven::write_dta(iv_2005_mod,"C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta")
```

## Moderate (targeted) inclusion data

Not looking for significance in the intercept, but rather significance in the variance of the intercept
-	BUT: variance can’t be negative, so standard significance tests aren’t of any help (log likelihood)

```{stata iv_2005_mod empty model, collectcode = TRUE}

use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta", clear

* dv num_victim_5yr will be replaced by total_security
* dv num_victim_5yr_winz will be replaced by security_winz

*1. empty model
mixed num_victim_5yr  || country:

estat icc   *intraclass correlation
*____________________________________________________________________
```

```{stata first iv_2005_mod model}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"

*2. First model
xtmixed num_victim_5yr gini_2004_6_cent gdppc_2004_6_scale police_eff_cent income_cent age_cent employed male [pw=individual_weight] || country:, pwscale(size)

*but actually
*xtmixed total_security num_victim_5yr gini_2004_6_cent gdppc_2004_6_scale police_eff_cent income_cent age_cent employed male [pw=individual_weight] || country:, pwscale(size)

*cook's D values
mltcooksd, fixed random graph

*intraclass correlation
estat icc

*BIC and AIC
estat ic

```

```{stata plots iv_2005_mod, echo = 1}
*use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta", clear

*plots
*scatter num_victim_5yr total_security, jitter(5)

*graph export scatter_iv_2005_mod.svg, replace
*___________________________________________________________________
```

```{stata winzorized model iv_2005_mod, collectcode = TRUE}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"

*3. Winzorized model (for reporting)
xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male police_eff_cent income_cent [pw=individual_weight] || country:, pwscale(size)

*but actually (with security variable)
*xtmixed security_winz gini_wc gdppc_2004_6_ws age_cent employed male police_eff_cent income_cent [pw=individual_weight] || country:, pwscale(size)

*security with nation mean
*xtmixed security_winz victim_nation_winz gini_wc gdppc_2004_6_ws age_cent employed male police_eff_cent income_cent [pw=individual_weight] || country:, pwscale(size)

estat icc
estat ic

mltcooksd, fixed random graph
*___________________________________________________________________
```


```{r create flags and update data iv_2005_mod}
# create dummy flags for outlier countries
iv_2005$extreme <- ifelse(iv_2005$country == "Peru" | iv_2005$country == "Argentina" | iv_2005$country == "United States", 1,0)

iv_2005$extreme1 <- ifelse(iv_2005$country == "Peru" | iv_2005$country == "Argentina" | iv_2005$country == "United States"| iv_2005$country == "South Africa", 1,0)

# create dummy flags for outlier countries
iv_2005_mod$extreme <- ifelse(iv_2005$country == "Peru" | iv_2005$country == "Argentina" | iv_2005$country == "United States", 1,0)

iv_2005_mod$extreme1 <- ifelse(iv_2005$country == "Peru" | iv_2005$country == "Argentina" | iv_2005$country == "United States"| iv_2005$country == "South Africa", 1,0)

haven::write_dta(iv_2005,"C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta")
haven::write_dta(iv_2005_mod,"C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta")
```


```{stata flag extreme lv2 model iv_2005_mod}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"

* 4. winz model, but with extreme countries flagged
set more off

xtmixed num_victim_5yr_winz extreme gini_wc gdppc_2004_6_ws age_cent employed male police_eff_cent income_cent [pw=individual_weight] || country:, pwscale(size)
*___________________________________________________________________
```


```{stata excluded iv_2005_mod model}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"

* 5. winzorized model, excluding extreme countires
set more off

gen dropcook=1 if country == "Peru" | country == "Argentina" | country ==  "United States"

xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male police_eff_cent income_cent [pw=individual_weight] || country: if dropcook !=1, pwscale(size)

estat icc
estat ic

mltcooksd, fixed random graph
*___________________________________________________________________
```

```{stata poisson}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"


*6. poisson model - pwscale not allowed - (note: Grid search failed to find values that will yield a log likelihood value.) - initial values not feasible
mepoisson num_victim_5yr gini_2004_6_cent gdppc_2004_6_scale police_effective income_quartile age_cent employed male [pw=individual_weight] || country:
*___________________________________________________________________


*7. poisson model without weights
mepoisson num_victim_5yr gini_2004_6_cent gdppc_2004_6_scale police_effective income_quartile age_cent employed male || country:
*___________________________________________________________________
```

### Exploratory

```{stata interaction model}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005_mod.dta"

*Winzorized model
xtmixed num_victim_5yr_winz c.gini_2004_6_wc##c.gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)

estat icc
estat ic

mltcooksd, fixed random graph
```

```{stata heteroscedasticity plot}
*use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/joined_sample.dta"

*Winzorized model
*xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)

*predict double xb, xb
*predict double re, residuals

*graph twoway scatter re xb, mcolor(black) msize(small) ylabel( , angle(horizontal) nogrid)
```

```{stata random effects plot}
xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)
```



## iv_2005 analyses

```{stata empty model iv_2005, collectcode = TRUE}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta", clear

mixed num_victim_5yr  || country:

estat icc   *intraclass correlation
```


```{stata first model iv_2005}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta"

*First model

xtmixed num_victim_5yr gini_2004_6_cent gdppc_2004_6_cent age_cent employed male [pw=individual_weight] || country:, pwscale(size)

mltcooksd, fixed random graph

estat icc   *intraclass correlation

estat ic       *BIC and AIC
```

Similar to the level-2 Cook's D's values on the lm model, argentian, peru, and the united states were all flagged as influential observations.

```{stata plots iv_2005, echo = 1}
*use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta", clear

*scatter num_victim_5yr total_security, jitter(5)

*graph export scatter_iv_2005.svg, replace
```

<!-- ![Mileage \~ Weight](scatter.svg) -->

```{stata winzorized model iv_2005, collectcode = TRUE}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta"

*Winzorized model
xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)

*xtmixed num_victim_5yr_winz victim_nation_mean gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)


estat icc
estat ic

mltcooksd, fixed random graph

```

```{stata excluded model iv_2005}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta"

set more off

gen dropcook=1 if country == "Peru" | country == "Argentina" | country ==  "United States"

*Winzorized model
xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country: if dropcook !=1, pwscale(size)

estat icc
estat ic

mltcooksd, fixed random graph
```


```{stata flag extreme lv2 model iv_2005}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta"

set more off

xtmixed num_victim_5yr_winz extreme gini_wc gdppc_2004_6_ws || country:
```

> When Ruiter and de Graaf (2010) excluded 23 cases based on DFBETAS, they found that their results held The point is that ultimately you need to have analyses that you can defend under scrutiny

> Some suggest absolute values of greater than 1 Cohen et al. (2003); Fox (2003) But Fox (2003) points out that this may not be effective large samples Cites additional research suggesting 2/(square root of n) This rule was also used by Van der Meer et al (2010:175) n refers to the n at the specific level, in this case level-2

> when I have around 30 clusters, I tend to depend on an absolute value of 1

> Unlike an OLS model, if we exclude a case at level-2, we also lose additional cases, in terms of all the cases at level-1 associated with that unit Van de Meer et al (2010) suggest retaining the level-1 cases, and then adding a dummy variable at level-2 to control for their unit That is, 0=not in excluded unit, 1=in excluded unit Also fix intercept to 0 for this cluster

```{stata excluded model1 iv_2005}
use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/iv_2005.dta"

set more off

gen dropcook=1 if country == "Peru" | country == "Argentina" | country ==  "United States" | country ==  "South Africa"

*Winzorized model
xtmixed num_victim_5yr_winz gini_2004_6_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country: if dropcook !=1, pwscale(size)

estat icc
estat ic


mltcooksd, fixed random graph

```

# reporting effects and models


# Poisson

<!-- https://errickson.net/stata-mixed/generalized-linear-mixed-models.html -->
Stata: Poisson mixed models can be run with the mepoisson command. A meqrpoisson command exists and has benefits just like meqrlogit, but again, is an outdated command. If over-dispersion is an issue, menbreg exists for negative binomial regression. 

# Negative binomial

menbreg num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)

menbreg



September 19, 2018

Log Likelihood
-	Cluster level variance is due to level 2 factors
o	What if you’re not interested in the direct effects of level 2 factors?
	You don’t need direct significance to preclude the use of a moderator


Tesging a coefficient
-	T or z = b/Sb
o	Sb = standard error of coefficient



when adding predictors, we can evaluate change in log likelihood to assess whether the fit is better
	chi-squarediff = -2(LLsimple – LLL)
-	why do we multiply by 2?
-	Degrees of freedom = difference in # of estimated parameters
-	You basically want to run a one-tailed test due to the inability to have a negative variance
o	Because we are testing the variance of the intercept



Chibar2(01)
-	Does the adjustment for the 1-tail test


Empirical bayes estimate
-	we bias group means towards population mean through “shrinkage”
-	subtracting some proportion of the observed group mean and substituting with an inverse proportion of the population mean
o	more within-group variance, and smaller sample size leads to greater shrinkage
BLUP – Best Linear Unbiased Prediction
-	This is the EB estimate


Predict groupeffect, reffects
-	Estimate error for each level-2 uit
-	This is the Blup/EB
-	Then, add grand mean with group error


So what do we report?
-	Bayesian Information Criterion (BIC) is quite conservative compared to significance tests	
o	BIC can be useful for comparing models
o	A non-parametric, relative measure of model fit
	Relative to another model (with a certain # of parameters and a certain amount of model fit), how does this fit?
	Is your fit worth the # of predictors you had to add to achieve it?
o	If BIC only goes down a little bit after introducing predictors, it might be better to use a more parsimonious model
o	Sine you’re just comp
o	Aring model fits/complexities, you don’t have to worry about inflating Type I Error
-	AIC Akaike Information Criterion



Generally, don’t use standardized slopes, instead use random slopes in MLM
-	When is standardization not useful?
o	Interpreting the effect as a substanstive issue
-	Random slope allows slope to vary across level 2
o	Level 2 moderates the coefficient
o	i.e., interaction
o	you can’t just use a difference in p-values to claim an interaction
	need to test these slopes (which are means)






there is
much to be gained when researchers follow a standardized
way of reporting effect sizes (Lumley et al., 2002).


### Survey Weights

The package lme4 fits mixed models when there are no weights or weights only for first-level units (Bates, Maechler, Bolker, & Walker, 2015)

> one does not need to de-emphasize responses from smaller nations because the MLM separates out variance due to responses from individuals in a country from the variance that comes from different nations. With the level-1 weight, the individual responses are representative of a nation, and the MLM then considers the cross-national context separately from this

> a scaled weight indicates that there was first a random sampling of clusters, and then a sampling of individuals in clusters. But, here, we looked at a set of randomly sampled people within specific nations

> Conceptually, we get around this by assuming that the characteristics we are examining are representative of a broader set of nations with similar characteristics But, for the purposes of weighting, there really isn't any probability of selection at level-2 to adjust by

> We essentially treat each nation as a whole equal unit, and then adjust only for sampling characteristics within the nation

Potential Packages: EdSurvey WeMix : appears to require two weights BIFIEsurvey : appears to allow one weight being used - but only level 2

The WeMix package assumes that the weights are unconditional---as is typical for sample weights found on international surveys such as PISA---but can accept conditional weights by setting the cWeights argument to TRUE.

<https://www.r-project.org/nosvn/pandoc/lme4.html>

> Recent/release notes Recent versions of lme4 (e.g. 1.1-6) give false convergence warnings. There is a summary post on r-sig-mixed-models. If you get warnings about max\|grad\| but the model passes this test: dd \<- [fit\@optinfo\$derivs](mailto:fit@optinfo$derivs){.email} with(dd,max(abs(solve(Hessian,gradient)))\<2e-3) then you are seeing a false-positive warning, and the problem will disappear in future versions (1.1-7 and up).


<https://m-clark.github.io/mixed-models-with-R/issues.html#convergence> \> Any complicated GLMM or similar model is likely to have problems, so be prepared. If you want to go beyond GLM, you'll have fewer tools and likely more issues. There are packages like ordinal, mgcv, glmmTMB, and others that can potentially handle alternate distributions and other complexities, however I think one might be better off with a Bayesian approach (e.g. brms/rstan). In practice, I've found others to be prohibitively slow, unable to converge, or too limited in post-estimation options.

Dr. B

> One must use weights when the sample design is "informative"(Koziol et al. 2017) The "design features---stratification, clustering, disproportionate-sampling---are associated with the response variable of interest" (Heeringa et al. 2010:59) One can render the design uninformative if controls render selection probabilities unrelated to outcome (Koziol et al. 2017)

> In the weights are uninformative, then one can omit the weights from model estimation (Heeringa et al. 2010:381) The issue here is that weights often account for more that just post-stratification, and these are likely to be associated with your outcome In other words, you will typically need to include weights in your MLM

On putting weights into MLM

> The issue here is that the weight we were using in design-based approaches is based on the individual probability of selection In other words, this weight includes adjustment for selection of clusters, such as PSU's (West et al. 2014) The problem here is that what we want is a weight that adjusts for selection, given the probability that the PSU was selected

> In other words, the sampling weight is the combination of the probability of a cluster being chosen and the individual in that cluster being chosen

> What we want then, is to isolate each weight. The cluster weight is the inverse probability that the weight was chosen

> The appropriate level-1 weight is then one's total inverse probability, scaled to the probability that one's cluster was chosen

> GET THE SCALED WEIGHT: This creates the inverse probability that one was chosen, relative to the inverse probability that their cluster was chosen Essentially scales one's overall probability to the cluster probability

> we can't just drop a weight created from a design-based approach into a model-based approach The bigger issue here is that many surveys do not provide information specifically on cluster-level probabilities of selection that would allow us to easily scale our design-based weights

> SCALING: Two approaches that are common are the PWIGLS method 2 and the MPML method A

> Chen (2018), a researcher with the Add Health study, argues in a presentation that the PWIGLS method is recommended when informative sampling methods are used in sampling both levels. In other words, if one does not have an SRS at both levels

> Stapleton (2013:379) indicates that both methods require sampling weights at each level, which is problematic if we don't have sampling weights for the PSU or cluster level

> If you do not have cluster-level weights, and you have a random sample of clusters, you are on more tenuous ground, but it may be possible to estimate the cluster-level weights

> Carl's (2009) simulation study concluded, "Scaled weighted estimates and standard errors differed slightly from unweighted analyses, agreeing more with each other than with unweighted analyses

> it appears more important to scale than use a very specific form of scaling

weights when there isn't a probability of level-2 selection

> This is therefore a case in which we simply drop in our level-1 weight without re-scaling We essentially treat each nation as a whole equal unit, and then adjust only for sampling characteristics within the nation

ICVS summary \> Results in this report are based on data which have been weighted to make the samples as representative as possible of national populations aged 16 or more in terms of 1. gender, 2. regional population distribution, 3. age, and 4. household composition.

> For this report, individual weights were used rather than household weights and each country carried equal weight in computing averages.

> More detail on the weighing procedure is given in appendix 6 and in the technical report on the Finnish retest with mobile-only users (Hideg, Manchin, 2007).



# Model comparison and selection - Field 868 - subtract log likelihood of new model from the value for the old

**Only works for ML estimateion (not REML)**
**Only works if new model contains "all of the effects of the older model"**
> "Importantly, the robustness of regression methods to deviations
from normality of the regression errors e does not only
depend on sample size, but also on the distribution of the
predictor X (Box & Watson, 1962; Mardia, 1971).
Specifically, when the predictor variable X contains a single
outlier, then it is possible that the case coincides with an outlier
in Y, creating an extreme observation with high leverage
on the regression line.
This is the only case where statistical
significance gets seriously misestimated based on the assumption
of Gaussian errors in Y which is violated by the outlier in
Y. This problem has been widely recognized (Ali & Sharma,
1996; Box&Watson, 1962; Miller, 1986; Osborne &Waters,
2002; Ramsey & Schafer, 2013; Zuur et al., 2010) leading to
the conclusion that Gaussian models are robust as long as
there are no outliers that occur in X and Y simultaneously."

Knief & Forstmeier, 2021

> Poisson distribution
- lambdas specify variance and mean of the distribution (doesn't have to be an integer)

> We have shown that Poisson models yielded heavily biased type I error rates (at α = 0.05) in either direction ranging from 0 to as high as 0.55 when their distribution assumption is violated (Fig. 3 right column, Fig. S7).

> Moreover, we worry that sophisticated methods
may allow presenting nearly anything as statistically significant
(Simmons et al., 2011) because complex methods will
only rarely be questioned by reviewers.



poisson assumptions
* rate at which events occur is constant
* occurence of one event does not affect occurence of a subsequent event

Probability Mass Function (PMF)
- how likely a given value is

cumulative distribution function
- all of the likelihoods of values up to and including the designated value

 GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA (https://cehs-research.github.io/eBook_multilevel/gee-count-outcome-epilepsy.html)
Match Poisson Regresssion (GLM)

restricted maximum likelihood (REML)

 Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. 

lmeresampler provides an easy way to bootstrap nested linear-mixed effects models using either the parametric, residual, cases, CGR (semi-parametric), or random effects block (REB) bootstrap fit using either lme4 or nlme. The output from lmeresampler is an lmeresamp http://aloy.github.io/lmeresampler/

bootMer 


https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/
Semi-parametric multilevel modeling (Tihomir Asparouhov)

 Probably the advice on this site will be to start with what kind of data your dependent variable is, and then choose an appropriate glm or regression approach. That is, if your dv is count, perhaps Poisson or negative binomial regression. If your dv is concentration, perhaps gamma regression. –
 

Note that the negative binomial and gamma distributions can only handle positive numbers, and the Poisson distribution can only handle positive whole numbers

# Limitations 

The most notable weakness is that this is data from WEIRD countries. Other, less WEIRD countries (e.g., phillipines, nigeria, india) are better represented throughout sweeps 2 to 4


Limited