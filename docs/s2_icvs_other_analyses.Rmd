---
title: "s2_icvs_other_analyses"
author: "Dallas Novakowski"
date: "05/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr) # pipe
library(purrr) # map function
library(car) #qqPlot

load("C:/Users/dalla/Google Drive/offline_data_files/icvs_pwt_swiid/data/icvs_joined_samples.RData")
```



## Procedure

The relationship between country-level inequality and the consumption of security products will be tested using a multilevel linear regression. This analysis will be accomplished using a combination of three archival datasets. 

Firstly, indicators of security consumption have been accessed from the International Crime Victimization survey (ICVS; REF), which  is an accumulation of standardized sample surveys to look at householders’ experiences with crime, policing, crime prevention and feelings of unsafety. Although it does not consist of longitudinal observations, the ICVS has been distributed across five phases over fifteen years (1989, 1992, 1996, 2000, 2005), totaling 300,000 people and 78 different countries (Data Archiving and Networked Services, n.d.). For the purposes of this study, it is most notable that this survey contains items such as respondents’ adopted measures to protect themselves against burglary.

Secondly, nation-level inequality in disposable income was accessed through the Standardized World Income Inequality Database (SWIID; REF). The SWIID has been designed to maximize the comparability of income inequality data while maintaining the widest possible coverage across countries and over time. As a result, the gini coefficients are accompanied by standard errors to reflect uncertainty in the estimates.

Lastly, countries' expenditure-side real GDP was retrieved from the Penn World Table, version 10.0 (PWT; REF). The selected GDP values are adjusted for Purchasing Power Parity, "to compare relative living standards across countries at a single point in time" (https://www.rug.nl/ggdc/productivity/pwt/?lang=en). Nations' GDP values were divided by their population sizes to yield a per-capita GDP value, which will be used for these analysis.

Although the above datasets extend across multiple years, the current analysis will be limited to observations across 2004-2006. Imposing this restriction allows for a sufficient number of countries, combined with relative recency of data. After a conservative exclusion process for missing data, survey sweeps conducted across 2004-6 yielded 31 countries with 85,085 participants, with a minimum of 790 participants per country. An exclusion strategy maximizing the number of variables yields 27 countries, and 48,671 participants.


# Analysis procedure

If the assumptions underlying a multilevel regression hold in this dataset, the analysis will be conducted using maximum likelihood estimates (MLE) in r package lme4 (REF). Despite the possibility of nonnormal data, the large sample size in this study will yield similar variance estimates to REML estimation.


Given that the SWIID is presented in a list of 100 different dataframes, the analysis will firstly be conducted on each dataframe, with the results stored separately.


```{r other text, warning=F, message=FALSE, echo=FALSE}
# expected value of the ML variance estimator is not equal to the true variance σ², although it approaches the true variance at large sample sizes
# https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#twolevelmodeling
# 
# The most common methods for estimating model parameters—both fixed effects and variance components—are maximum likelihood (ML) and restricted maximum likelihood (REML)
# 
# REML is conditional on the fixed effects, so that the part of the data used for estimating variance components is separated from that used for estimating fixed effects. Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. REML is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects. ML should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects (with the same fixed effects model).
# 
# assume that random effects follow a normal distribution with mean 0 and a variance parameter which must be estimated from the data.
# 
# the error terms at Level Two can be assumed to follow a multivariate normal distribution
# 
# Certain software packages will report p-values corresponding to hypothesis tests for parameters of fixed effects; these packages are typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution. 

# 
# 
# ML and REML assume errors a normally distributed
# - differences between the two estimation methods occur in variance estimation: at large n-to IV ratios, immaterial difference
# 
# REML preferable as its variance parameters are unbiasesd
# 
# however, ML better for carrying out deviance tests
```

```{r estimation preview1, warning=F, message=FALSE}
# Estimate model
# m1 <- icvs_joined %>% 
#   map(~ lme4::lmer(num_victim_5yr ~ gini_2004_6_cent + gdppc_2004_6_cent + 
#                      age_cent + employed + male + (1 | country),data = .x, REML=FALSE))


# Both packages use Lattice as the backend, but nlme has some nice features like groupedData() and lmList() that are lacking in lme4 (IMO). From a practical perspective, the two most important criteria seem, however, that

# https://lme4.r-forge.r-project.org/book/

#    lme4 extends nlme with other link functions: in nlme, you cannot fit outcomes whose distribution is not gaussian, lme4 can be used to fit mixed-effects logistic regression, for example.
#    in nlme, it is possible to specify the variance-covariance matrix for the random effects (e.g. an AR(1)); it is not possible in lme4.

#Now, lme4 can easily handle very huge number of random effects (hence, number of individuals in a given study) thanks to its C part and the use of sparse matrices. The nlme package has somewhat been superseded by lme4 so I won't expect people spending much time developing add-ons on top of nlme. Personally, when I have a continuous response in my model, I tend to use both packages, but I'm now versed to the lme4 way for fitting GLMM.

# nlme let's you specify variance-covariance structures for the residuals (i.e. spatial or temporal autocorrelation or heteroskedasticity), lme4 doesn't

# https://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models
# lmeg or nlme?
#nlme is the very large tool box, including a TIG welder to make any tools you need.

# brms: Bayesian Regression Models using 'Stan'
# https://cran.r-project.org/web/packages/brms/index.html
```

Each of these 100 dataframes will then be subjected to 100 simulations, yielding vectors of 10,000 estimates. The means and distributions of these vectors will yield the coefficient and standard error values, respectively.

"We could now use Rubin’s (1987) rules to calculate the mean and standard error for each fixed effect estimate across these 100 sets of results (these rules are implemented in the package mitools (Lumley 2014) and others), but instead we will use simulation. Using sim from the arm package (Gelman and Su 2015), we generate the distribution of fixed-effects estimates in the results both for each dataframe and across the 100 dataframes."

Here we use the dotwhisker package (Solt and Hu 2015) to present a dot-and-whisker plot of the results. The dots represent the estimated change on the ten-point religiosity scale for a change of two standard deviations in each independent variable; the whiskers represent the 95% confidence intervals of
these estimates.


# reporting effects and models

What can an empty model tell us?
-	Intraclass correlation

Not looking for significance in the intercept, but rather significance in the variance of the intercept
-	BUT: variance can’t be negative, so standard significance tests aren’t of any help


September 19, 2018

Log Likelihood
-	Cluster level variance is due to level 2 factors
o	What if you’re not interested in the direct effects of level 2 factors?
	You don’t need direct significance to preclude the use of a moderator


Tesging a coefficient
-	T or z = b/Sb
o	Sb = standard error of coefficient



when adding predictors, we can evaluate change in log likelihood to assess whether the fit is better
	chi-squarediff = -2(LLsimple – LLL)
-	why do we multiply by 2?
-	Degrees of freedom = difference in # of estimated parameters
-	You basically want to run a one-tailed test due to the inability to have a negative variance
o	Because we are testing the variance of the intercept

Chibar2(01)
-	Does the adjustment for the 1-tail test


Empirical bayes estimate
-	we bias group means towards population mean through “shrinkage”
-	subtracting some proportion of the observed group mean and substituting with an inverse proportion of the population mean
o	more within-group variance, and smaller sample size leads to greater shrinkage
BLUP – Best Linear Unbiased Prediction
-	This is the EB estimate


Predict groupeffect, reffects
-	Estimate error for each level-2 uit
-	This is the Blup/EB
-	Then, add grand mean with group error


So what do we report?
-	Bayesian Information Criterion (BIC) is quite conservative compared to significance tests	
o	BIC can be useful for comparing models
o	A non-parametric, relative measure of model fit
	Relative to another model (with a certain # of parameters and a certain amount of model fit), how does this fit?
	Is your fit worth the # of predictors you had to add to achieve it?
o	If BIC only goes down a little bit after introducing predictors, it might be better to use a more parsimonious model
o	Sine you’re just comp
o	Aring model fits/complexities, you don’t have to worry about inflating Type I Error
-	AIC Akaike Information Criterion



Generally, don’t use standardized slopes, instead use random slopes in MLM
-	When is standardization not useful?
o	Interpreting the effect as a substanstive issue
-	Random slope allows slope to vary across level 2
o	Level 2 moderates the coefficient
o	i.e., interaction
o	you can’t just use a difference in p-values to claim an interaction
	need to test these slopes (which are means)






there is much to be gained when researchers follow a standardized way of reporting effect sizes (Lumley et al., 2002).

# Assumption checks and and contingencies

<ul>**1. Linearity**</ul>
  * **Check:** Scatterplots
  * **Fix:** add polynomials until adequate fit to linear function is obtained


    
    
Nonlinearity does not seem to be a major issue in the data. Especially with a small sample, adding polynomials seems more likely to risk overfitting the data. If anything, these charts suggest that these trends might be largely driven by a few influential observations.

```{r cooksd, warning=F, message=FALSE, echo=FALSE}
# influential cases (Cook's Distance. Any values over 1 are likely to be significant outliers)
cooksD <- cooks.distance(m1_sample)
summary(cooksD)


# look for influential observations in nation-level
nation_cook <- cooks.distance(lm(gini_2004_6 ~ gdppc_2004_6, gd))
summary(nation_cook)
plot(nation_cook)
abline(h = 4/nrow(gd), lty = 2, col = "steelblue") # add cutoff line


# influential observations for gini
victim_gini_cook <- cooks.distance(lm(num_victim_5yr ~ gini_2004_6, gd))
summary(victim_gini_cook)
plot(victim_gini_cook)
abline(h = 4/nrow(gd), lty = 2, col = "steelblue") # add cutoff line

# plot with outliers
victim_gini_outlier <- ggplot(gd, aes(x = gini_2004_6, y = num_victim_5yr)) +
    geom_point() + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + 
    theme_minimal() + ggtitle("Victimization & Gini - Outliers") + scale_y_continuous(limits = c(0, 1.25))

# plot without outliers
victim_gini_no_outlier <- ggplot(filter(gd, gini_2004_6 < 50), aes(x = gini_2004_6, y = num_victim_5yr)) +
    geom_point() + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + 
    theme_minimal() + ggtitle("Victimization & Gini - No Outliers")  + scale_y_continuous(limits = c(0, 1.25))

#plot the two scatterplots side by side
gridExtra::grid.arrange(victim_gini_outlier,victim_gini_no_outlier, ncol=2)


# cooks distance for gdp
victim_gdp_cook <- cooks.distance(lm(num_victim_5yr ~ gdppc_2004_6, gd))
summary(victim_gdp_cook)
plot(victim_gdp_cook)
abline(h = 4/nrow(gd), lty = 2, col = "steelblue") # add cutoff line

# plot with outliers
victim_gdp_outlier <- ggplot(gd, aes(x = gdppc_2004_6, y = num_victim_5yr)) +
    geom_point() + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + 
    theme_minimal() + ggtitle("Victimization & GDP - Outliers")  + scale_x_continuous(limits = c(1500, 85000)) + scale_y_continuous(limits = c(.25, 1.25))
# + geom_smooth(method = "lm", formula = y~log(x), alpha = .25, color = "green")

# plot without outliers
victim_gdp_no_outlier <- ggplot(filter(gd, gdppc_2004_6 < 60000), aes(x = gdppc_2004_6, y = num_victim_5yr)) +
    geom_point() + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + 
    theme_minimal() + ggtitle("Victimization & GDP - No Outliers")  + scale_x_continuous(limits = c(1500, 85000)) + scale_y_continuous(limits = c(.25, 1.25))

#plot the two scatterplots side by side
gridExtra::grid.arrange(victim_gdp_outlier, victim_gdp_no_outlier, ncol=2)

```


However, the cooks distance values are all well under 1. Importantly, this is the same for the nation-level variables; they have a smaller sample size - and are thus the more susceptible to influential observations.



```{r linearity1, warning=F, message=FALSE, echo=FALSE}
# ggplot(join_sample, aes(x = age_cent, y = num_victim_5yr)) +
#     geom_point(alpha = .02, position = "jitter", color = "#40B0A6") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + 
#     theme_minimal()
```



```{r gdpgini scatter, warning=F, message=FALSE, echo=FALSE}

#create scatterplot for data frame with no outliers
no_outliers_plot <- ggplot(filter(gd,gini_2004_6 < 50), aes(x = gini_2004_6, y = gdppc_2004_6)) +
    geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + ggtitle("No Outliers")  + scale_y_continuous(limits = c(-1000, 85000))

#create scatterplot for data frame with outliers
outliers_plot <- ggplot(gd, aes(x = gini_2004_6, y = gdppc_2004_6)) +
    geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + ggtitle("With Outliers")  + scale_y_continuous(limits = c(-1000, 85000))

#plot the two scatterplots side by side
gridExtra::grid.arrange(no_outliers_plot, outliers_plot, ncol=2)


ggplot(gd, aes(x = gini_2004_6, y = gdppc_2004_6)) +
    geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25)

# there is so much skew in this data that it does not do much to inform linearity judgments
plot(resid(m1_sample),mod_joined1$num_victim_5yr[1:length(resid(m1_sample))])
```

    
<ul>**2. Normality in residuals**</ul>
  * **Check**: QQplots (The Normal Probability Plot method.)
    * Levene’s test - this is more robust to departures from normality than Bartlett’s test. It is in the car package.
    * Fligner-Killeen test - this is a non-parametric test which is very robust against departures from normality.
  * **NO treatment:** Lumley, Diehr, Emerson, & Chen (2002) provided simulation evidence that linear regressions conducted on large samples are robust to departures from normality. With at least 500 observations, they found that linear regression can be conducted with negligible impact on Type I errors. Likewise, Knief & Forstmeier (2021) found  that for anything other than very small sample sizes, "p-values from Gaussian models are highly robust to even extreme violation of the normality assumption and can be trusted, except when involving X and Y distributions with extreme outliers (p. ??). Likewise, "for N = 1000, power was essentially unaffected by the distribution of Y and X."

Finally, the authors found a Gaussian error distribution to be far more robust to violations of its assumptions than Poisson and Binomial distributions. Initially, a binomial or poisson process seems to better describe the distribution of this study's dependent variable (both for the proposed and presented analyses). However, these variables likely violate the assumption of event independence, where one event does not affect the likelihood of a subsequent event. For instance, in the case of security consumption, the psychological and material impacts of buying a security product (e.g., feelings of safety, reduced budget), can plausibly impact a consumers' decision to purchase further security goods. Given the potential for heavy bias in poisson and negative binomial models, a gaussian error structure's comparative robustness appears to make a (violated) assumption of normality be the most appropriate choice.


> Moreover, we worry that sophisticated methods
may allow presenting nearly anything as statistically significant
(Simmons et al., 2011) because complex methods will
only rarely be questioned by reviewers.


```{r Normality, warning=F, message=FALSE}
qqnorm(resid(m1_sample))
qqline(resid(m1_sample), col = "darkgreen")

plot(density(resid(m1_sample)))


moments::skewness(resid(m1_sample))
moments::kurtosis(resid(m1_sample))


```


Indeed, this is dependent variable is definitely not normal, with a skewness of `r round(moments::skewness(resid(m1_sample)),2)`, and a kurtosis of *`r round(moments::kurtosis(resid(m1_sample)),2)`*

With high sample sizes, their analyses found that even dramatic violations of normality assumptions (gaussian and binomial distributions) had no impact on Type I errors

```{r poigam, warning=F, message=FALSE, echo=FALSE, results='hide'}
# https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/

# # unexpected missingness exclusion
# summary(!is.na(join_sample[["num_victim_5yr"]]))
# which(is.na(join_sample[["num_victim_5yr"]]))
# join_sample <- join_sample[-c(29052),] 
# 

# join_sample$num_victim_5yr_1 <- join_sample$num_victim_5yr + 1
# 
# norm <- MASS::fitdistr(join_sample$num_victim_5yr_1, "normal")
# 
# # qqp(join_sample$num_victim_5yr, "norm", main = "Normal distribution")
# 
# qqp(join_sample$sqrt_victimization, "norm", main = "Normal distribution, square root")
# 
# qqp(join_sample$cubrt_victimization, "norm", main = "Normal distribution, cube root")
# 
# qqp(join_sample$num_victim_5yr_1, "lnorm", main = "Logged normal distribution")
# 

# negative binomial - fails to optimize with +1 transformation
# nbinom <- MASS::fitdistr(join_sample$num_victim_5yr_1, "Negative Binomial")
# qqp(join_sample$num_victim_5yr_1, "nbinom", main = "Negative binomial distribution", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])


#poisson distribution
poisson <- MASS::fitdistr(mod_joined1$num_victim_5yr, "Poisson")
qqp(mod_joined1$num_victim_5yr, "pois", main = "Poisson distribution", lambda = poisson$estimate)


gamma <- MASS::fitdistr(mod_joined1$num_victim_5yr+1, "gamma")
qqp(mod_joined1$num_victim_5yr+1, "gamma", main = "Gamma distribution", shape = gamma$estimate[[1]], rate = gamma$estimate[[2]])

# lattice::qqmath(m1_sample, id=0.05)
```




<ul>**3. Multicolinnearity**</ul>
  * **Check**: VIF/tolerances
  * **Fix** remove values >= 5 
     
```{r VIF1, warning=F, message=FALSE}
car::vif(m1_sample) # display VIF values
```
        
4. HETEROSCEDASTICITY
     
  * **Check**: Plot residuals vs predicted values, and residuals versus independent variables
    * Others: Breush-Pagan test and the NCV test, levene's test
  * **Fix** with Box-Cox transformation of DV; caret::BoxCoxTrans(yvar), log transformation of DV
  * boxcoxmix packag
  
>  There is a two-parameter version of the Box-Cox transformation that allows a shift before transformation:\The usual Box-Cox transformation sets λ2=0. One common choice with the two-parameter version is λ1=0 and λ2=1 which has the neat property of mapping zero to zero. There is even an R function for this: log1p().  More generally, both parameters can be estimated. In R, the boxcox.fit() function in package geoR will fit the parameters.
  
 > * Inverse hyperbolic sine (IHS) transformation, mixture models (https://robjhyndman.com/hyndsight/transformations/)
Other options: 
  *  we want to obtain heteroskedasticity robust standard errors and their corresponding t values. In R the function coeftest from the lmtest package can be used in combination with the function vcovHC from the sandwich package to do this. 
  
  https://stackoverflow.com/questions/48479984/r-mixed-model-with-heteroscedastic-data-only-lm-function-works
  
>  The sandwich package is not limited to lm/glm only but it is in principle object-oriented, see vignette("sandwich-OOP", package = "sandwich") (also published as doi:10.18637/jss.v016.i09.
>
> There are suitable methods for a wide variety of packages/models but not for nlme or lme4. The reason is that it's not so obvious for which mixed-effects models the usual sandwich trick actually works. (Disclaimer: But I'm no expert in mixed-effects modeling.)
> 
>  However, for lme4 there is a relatively new package called merDeriv (https://CRAN.R-project.org/package=merDeriv) that supplies estfun and bread methods so that sandwich covariances can be computed for lmer output etc. There is also a working paper associated with that package: https://arxiv.org/abs/1612.04911
  
  * https://www.r-econometrics.com/methods/hcrobusterrors/

> For a heteroskedasticity robust F test we perform a Wald test using the waldtest function, which is also contained in the lmtest package. It can be used in a similar way as the anova function, i.e., it uses the output of the restricted and unrestricted model and the robust variance-covariance matrix as argument vcov. Based on the variance-covariance matrix of the unrestriced model we, again, calculate White standard errors.

knief and Forstmeier, 2021
> Most elegantly,heteroscedasticity can be modeled directly, for instance by
using the “weights” argument in lme (see Pinheiro & Bates,
2000, p. 214), which also enables us to test directly whether
allowing for heteroscedasticity increases the fit of the model
significantly. Similarly, heteroscedasticity-consistent standard
errors could be estimated (Hayes & Cai, 2007). For more
advice on handling heteroscedasticity, see McGuinness
(2002).
     
> we did not cover collinearity between predictors or the distribution of random effects, but others have dealt with these
aspects before (Freckleton, 2011; Schielzeth et al., 2020).

```{r heteroscedasticity1, warning=F, message=FALSE}
levene_data<- mod_joined1[1:length(resid(m1_sample)),]

levene_data$model_res<- residuals(m1_sample)
levene_data$model_res_abs <-abs(levene_data$model_res) #creates a new column with the absolute value of the residuals
levene_data$model_res_abs2 <- levene_data$model_res_abs^2 #squares the absolute values of the residuals to provide the more robust estimate
levene_m1 <- lm(model_res_abs2 ~ country, data=levene_data) #ANOVA of the squared residuals
anova(levene_m1) #displays the results

# par(mfrow=c(2,2))

# plot(m1_sample, which = 1)

plot(m1_sample, resid(., type = "pearson") ~ fitted(.), abline = 0)

plot(density(resid(m1_sample)))


# https://www.r-econometrics.com/methods/hcrobusterrors/
# obtain heteroskedasticity robust standard errors and their corresponding t values. - no applicable method for 'estfun' applied to an object of class "c('lmerMod', 'merMod')"
# lmtest::coeftest(m1_sample, vcov = sandwich::vcovHC(m1_sample, type = "HC0"))

# Breush Pagan Test - doesn't seem to work for lmer models    https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/
# lmtest::bptest(m1_sample)

# NCV test - definitely doesn't work for lmer
# car::ncvTest(m1_sample) 

#varPower() might have something to do with improving homoscedasticity

# So I like to use heteroscedastic-consistent covariance matrices. This paper explains it well. The sandwich and lmtest packages are great. Here is a good explanation how to do it for a indpendent design in R with lm(y ~ x). 
# https://stackoverflow.com/questions/48479984/r-mixed-model-with-heteroscedastic-data-only-lm-function-works
```


```{r regression assumptions}

# 1. linearity :DV/IV


# 2. HOMOSCEDASTICITY: the variance of errors is the same across all levels of the IV


#     Bartlett’s test - If the data is normally distributed, this is the best test to use. It is sensitive to data which is not non-normally distribution; it is more likely to return a “false positive” when the data is non-normal.

# 
# 
# 3. normality 
#QQ plots # S3 method for lm
# qqPlot(m1_sample, xlab=paste(distribution, "Quantiles"),
#     ylab=paste("Studentized Residuals(",
#                 deparse(substitute(m1_sample)), ")", sep=""),
#     main=NULL, distribution=c("t", "norm"),
#     line=c("robust", "quartiles", "none"), las=par("las"),
#     simulate=TRUE, envelope=TRUE,  reps=100,
#     col=carPalette()[1], col.lines=carPalette()[2], lwd=2, pch=1, cex=par("cex"),
#     id=TRUE, grid=TRUE, ...)
# 
# qqPlot(m1_sample)

#  independent residuals

# ggplot(m1_sample, aes(sample=.resid)) +
#     stat_qq()

# random coefficients are normally distributed in overall model

# reliability (of measurement)

# residuals are independent (Durbin-Watson)

# Multivariate Normality values of the residuals are normally distributed. (P-P plot)

# multicollinearity


```


# Model comparison and selection - Field 868 - subtract log likelihood of new model from the value for the old

**Only works for ML estimateion (not REML)**
**Only works if new model contains "all of the effects of the older model"**
> "Importantly, the robustness of regression methods to deviations
from normality of the regression errors e does not only
depend on sample size, but also on the distribution of the
predictor X (Box & Watson, 1962; Mardia, 1971).
Specifically, when the predictor variable X contains a single
outlier, then it is possible that the case coincides with an outlier
in Y, creating an extreme observation with high leverage
on the regression line.
This is the only case where statistical
significance gets seriously misestimated based on the assumption
of Gaussian errors in Y which is violated by the outlier in
Y. This problem has been widely recognized (Ali & Sharma,
1996; Box&Watson, 1962; Miller, 1986; Osborne &Waters,
2002; Ramsey & Schafer, 2013; Zuur et al., 2010) leading to
the conclusion that Gaussian models are robust as long as
there are no outliers that occur in X and Y simultaneously."

Knief & Forstmeier, 2021

> Poisson distribution
- lambdas specify variance and mean of the distribution (doesn't have to be an integer)

> We have shown that Poisson models yielded heavily biased type I error rates (at α = 0.05) in either direction ranging from 0 to as high as 0.55 when their distribution assumption is violated (Fig. 3 right column, Fig. S7).

> Moreover, we worry that sophisticated methods
may allow presenting nearly anything as statistically significant
(Simmons et al., 2011) because complex methods will
only rarely be questioned by reviewers.



poisson assumptions
* rate at which events occur is constant
* occurence of one event does not affect occurence of a subsequent event

Probability Mass Function (PMF)
- how likely a given value is

cumulative distribution function
- all of the likelihoods of values up to and including the designated value

 GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA (https://cehs-research.github.io/eBook_multilevel/gee-count-outcome-epilepsy.html)
Match Poisson Regresssion (GLM)

restricted maximum likelihood (REML)

 Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. 

lmeresampler provides an easy way to bootstrap nested linear-mixed effects models using either the parametric, residual, cases, CGR (semi-parametric), or random effects block (REB) bootstrap fit using either lme4 or nlme. The output from lmeresampler is an lmeresamp http://aloy.github.io/lmeresampler/

bootMer 


https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/
Semi-parametric multilevel modeling (Tihomir Asparouhov)

 Probably the advice on this site will be to start with what kind of data your dependent variable is, and then choose an appropriate glm or regression approach. That is, if your dv is count, perhaps Poisson or negative binomial regression. If your dv is concentration, perhaps gamma regression. –
 

Note that the negative binomial and gamma distributions can only handle positive numbers, and the Poisson distribution can only handle positive whole numbers

# Limitations 

The most notable weakness is that this is data from WEIRD countries. Other, less WEIRD countries (e.g., phillipines, nigeria, india) are better represented throughout sweeps 2 to 4


Limited

## On the distribution

The population distribution for our dependent variables is not known in advance. All we can say is that we know it is a count variable. This uncertainty opens up the possibility of a model selection bias.

The data-generating processes we could consider here are 

1. [normal](#assuming-a-normal-distribution), 
2. [poisson and negative binomial](#poisson-distribution), 
3. an ordinal process.
4. A three-level item-response-style model (item|person|country)

<!-- [three-level logistic regression](#three-level-logistic-regression). -->

At first glance, victimization looks like a poisson process, but we have limited reason to believe that our outcome variables (e.g., security, number of victimizations) follow a strictly poisson process. Firstly, the count may violate the assumption of independent events; I would guess that purchasing security or being victimized by a crime has *some* effect on later occurrences in either direction (e.g., having less money from the prior security purchase).

Secondly, poisson assumes unbounded counts, where the max value stretches out indefinitely (albeit with smaller and smaller frequency). Our count variables are the result of summing a finite number of binary responses.

<!-- One note is that victimizations seem to have inflated zeros. Another is that both variables have larger variances than means - more indicative of negative binomial. A question though - is the independence supposed to be observed within observations or between observations? -->

All told, let's remember that the assumptions of normality apply to the distribution of residuals, so can only be interpreted in the model. Important to keep skewness of the DV on our radar though.

Starting with scatterplots. Nation Values with a cook's distance of 4 times the mean cook's distance values (for bivariate relationship) are flagged.




# Multilevel ordinal logistic regression

The high degree of skewness, and complete lack of symmetry in the DV distributions, poses some issues for modelling. A good example is out-of sample predictions, since we are using a count variable, negative predicted values are a real concern.

Situations that completely rule out the use of linear methods are: edge effects in the data (i.e. many data points in the highest/lowest category)

When selecting a distribution process, a categorical logistic is obviously more parsimonious. However, a binary or multinomial logistic process in this case misses some meaningful structure to the responses. Firstly, since our DVs are the summed total of binary variables, there is definitely a hierarchy to the values; 1 is larger than 0, and so on.

Secondly, there is likely a meaningful difference between the values of 1, 2, and 3. The argument being that people can stumble into their first unit of security consumption, where they buy a house that happens to have special locks or grates on the window, but those second and third units are more likely to be a deliberate choice to consume security goods.\
- Note: this may be a violation of the proportional odds assumption - "that the relationship between each pair of outcome groups has to be the same" - If this assumption is violated, different models are needed to describe the relationship between each pair of outcome groups. - Namely, we'd expect stronger effects at higher levels of the outcome

## doing ordinal

<https://kevinstadler.github.io/notes/bayesian-ordinal-regression-with-random-effects-using-brms/>





ordered logistic (or more accurately ordered logit) regression is an extension of logistic/logit regression: where in logistic regression you model one coefficient that captures the relative likelihood (in log-odds) of one outcome occurring over another (i.e. 2 outcomes captured by 1 coefficient), ordered logit regression models the relative likelihood of k different outcomes based on k-1 coefficients. The k-1 coefficients in the model capture the cumulative likelihood of responses falling into an expanding set of ordered response categories, e.g. in the case of 4 possible outcome categories (o1-o4) the three intercepts of the model, each of which will be at least as high as the preceding one, capture the baseline log-odds of observing:

log(o1 vs. o2, o3, o4) log(o1, o2 vs. o3, o4) log(o1, o2, o3 vs. o4)

The choice of link function is typically not critical and most methods assume the "logit" function (the log-odds transformation that forms the basis of ordered logit regression) by default, but a different choice can be informed by your knowledge of the data.



### ordinal assumptions

<https://medium.com/evangelinelee/ordinal-logistic-regression-on-world-happiness-report-221372709095>

    The dependent variable are ordered.
    One or more of the independent variables are either continuous, categorical or ordinal.
    No multi-collinearity.
    Proportional odds

    Now we should conduct the Brant Test to test the last assumption about proportional odds. This assumption basically means that the relationship between each pair of outcome groups has to be the same. If the relationship between all pairs of groups is the same, then there is only one set of coefficient, which means that there is only one model. If this assumption is violated, different models are needed to describe the relationship between each pair of outcome groups.




# Assumption checks and and contingencies

## Influential observations

-   **Check:** cook's D values exceeding the threshold 4/n; where n = \# of clusters.
-   **Treatment:** winzorize variables that demonstrate exhibit sufficient influence on model
-   **Robustness check:**

a.  retain the level-1 cases of extreme values, add a dummy variable at level-2 to control for their unit, rerun analysis (Van de Meer et al, 2010). Document whether any coefficients change.
b.  

run analyses excluding cases that exceed the threshold 4/n; where n = \# of clusters. Following, evaluate cook's D values on this reduced dataset. Eliminate any cases that exceed a cook's d of .5, and conduct a third analysis.

## Linearity

-   **Check:** Scatterplots for nonlinear relationships
-   **Treatment:** If robust nonlinear relationship detected, add polynomials until adequate fit to linear function is obtained

Nonlinearity does not seem to be a major issue in the data. There may be some pattern of as much as a third-order polynomial in the effects of gini on victimization, (second-order for GDP).

Especially with a small sample at level-2, adding polynomials seems more likely to risk overfitting the data. I would only assess the fit of a polynomial model if I could then test it on another dataset, for cross-validation.

## Normality in residuals

-   **Check**: QQplots (The Normal Probability Plot method.) for whether lie on normal distribution

**NO treatment:** Although there are many possible transformations possible for this kind of data (square root, rankit/INT, log(x+1), boxcox(x+1)), nonnormal data, by itself, is not expected to adversely impact the proposed analysis. Lumley, Diehr, Emerson, & Chen (2002) provided simulation evidence that linear regressions conducted on large samples are robust to departures from normality. With at least 500 observations, they found that linear regression can be conducted with negligible impact on Type I errors. Likewise, Knief & Forstmeier (2021) found that for anything other than very small sample sizes, "p-values from Gaussian models are highly robust to even extreme violation of the normality assumption and can be trusted, except when involving X and Y distributions with extreme outliers (p. ??). Likewise, "for N = 1000, power was essentially unaffected by the distribution of Y and X."

```{r other distributions, warning=F, message=FALSE, echo=FALSE, results='hide', fig.show="hold", out.width="50%"}
# https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/

# # unexpected missingness exclusion
# summary(!is.na(iv_2005[["num_victim_5yr"]]))
# which(is.na(iv_2005[["num_victim_5yr"]]))

iv_2005$num_victim_5yr_winz_1 <- iv_2005$num_victim_5yr_winz + 1

norm <- MASS::fitdistr(iv_2005$num_victim_5yr_winz_1, "normal")

# qqp(iv_2005$num_victim_5yr, "norm", main = "Normal distribution")

qqp(iv_2005$num_victim_5yr_winz_1, "norm", main = "Normal distribution")


qqp(sqrt(iv_2005$num_victim_5yr_winz), "norm", main = "Normal distribution, square root")
# qqp(sqrt(iv_2005$num_victim_5yr_winz), "norm", main = "winzorized and square root")

qqp(iv_2005$num_victim_5yr_winz_1, "lnorm", main = "Logged normal distribution")
# qqp(iv_2005$num_victim_5yr_winz + 1, "lnorm", main = "Winzorized and Logged")

# negative binomial - fails to optimize with +1 transformation
# nbinom <- MASS::fitdistr(iv_2005$num_victim_5yr_1, "Negative Binomial")
# qqp(iv_2005$num_victim_5yr_1, "nbinom", main = "Negative binomial distribution", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])


#poisson distribution
poisson <- MASS::fitdistr(iv_2005$num_victim_5yr_winz_1, "Poisson")
qqp(iv_2005$num_victim_5yr_winz_1, "pois", main = "Poisson distribution", lambda = poisson$estimate)


#poisson modeldistribution
poisson <- MASS::fitdistr(iv_2005$num_victim_5yr_winz_1, "Poisson")
qqp(iv_2005$num_victim_5yr_winz_1, "pois", main = "Poisson distribution", lambda = poisson$estimate)


# qqp(iv_2005$num_victim_5yr_winz+1, "pois", main = "Winzorized Poisson distribution", lambda = poisson$estimate)


# lattice::qqmath(m1_sample, id=0.05)
```

**On alternate error distributions**

Initially, a poisson process seems to better describe the distribution of this study's dependent variable. However, the variables *num_victim_5yr* and *security_total* each likely violate the assumption of event independence, where one event does not affect the likelihood of a subsequent event. For instance, in the case of security consumption, the psychological and material impacts of buying a security product (e.g., feelings of safety, reduced budget), can plausibly impact a consumers' decision to purchase further security goods. Knief & Forstmeier (2021) found a Gaussian error distribution to be far more robust to violations of its assumptions than Poisson and Binomial distributions. Given the potential for heavy bias in poisson and negative binomial models, a gaussian error structure's comparative robustness appears to make a (violated) assumption of normality be the most appropriate choice compared to a possibly misattributed poisson distribution.

Knief & Forstmeier, 2021

> Poisson distribution - lambdas specify variance and mean of the distribution (doesn't have to be an integer)

> We have shown that Poisson models yielded heavily biased type I error rates (at α = 0.05) in either direction ranging from 0 to as high as 0.55 when their distribution assumption is violated (Fig. 3 right column, Fig. S7).

> Moreover, we worry that sophisticated methods may allow presenting nearly anything as statistically significant (Simmons et al., 2011) because complex methods will only rarely be questioned by reviewers.

> Moreover, we worry that sophisticated methods may allow presenting nearly anything as statistically significant (Simmons et al., 2011) because complex methods will only rarely be questioned by reviewers.

DFBeta values: difference between estimate including and deleting outlier

DFFit: difference between adjusted predicted value and original value

## Heteroscedasticity (Homogeneity of Variance)

-   **Check**: Plot residuals vs predicted values, for changes in residual variance across levels of predicted values

```{stata heteroscedasticity plot}

*use "C:/Users/dalla/Google Drive/data_files/icvs_pwt_swiid/data/joined_sample.dta"


*Winzorized model
*xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)


*predict double xb, xb
*predict double re, residuals

*graph twoway scatter re xb, mcolor(black) msize(small) ylabel( , angle(horizontal) nogrid)

```

-   **Treatment:** Transformation of DV (e.g., box-cox, square root, log, rankit). Beyond this, the model already has robust standard errors

```{r victimization transformation}

# Square root method
iv_2005$sqrt_victimization <- sqrt(iv_2005$num_victim_5yr)
  # normality_stats(iv_2005$sqrt_victimization)

# Cube root method
iv_2005$cubrt_victimization <- '^'(iv_2005$num_victim_5yr,1/3) # cube root transformation
  #normality_stats(iv_2005$cubrt_victimization)

# rankit method
iv_2005$rankit_victimization <- RNOmni::RankNorm(iv_2005$num_victim_5yr) # rank-based inverse normal transformation (approximately normalize most distribtutional shapes, and which effectively minimizes type I errors and maximizes statistical power; Bishara & Hittner, 2012; Puth et al., 2014
  #normality_stats(iv_2005$rankit_victimization)

#ggplot(iv_2005, aes(x = rankit_victimization))  +  geom_histogram(binwidth= 1, alpha=.7, fill="#40B0A6", colour='grey') + geom_density(adjust=6, aes(y=..count..)) + theme_minimal()

# Ordinal method
iv_2005$ordinal_victimization <- NA
iv_2005$ordinal_victimization <- ifelse(between(iv_2005$num_victim_5yr,0,1), 1, NA)
iv_2005$ordinal_victimization[iv_2005$num_victim_5yr == 0] <- 0 
iv_2005$ordinal_victimization[iv_2005$num_victim_5yr > 1] <- 2

# Log method
iv_2005$log_victimization <- log10(iv_2005$num_victim_5yr+1)
  #normality_stats(iv_2005$log_victimization)


```

```{r sqrt security, warning=F, message=FALSE, echo=FALSE}
# Square root method
iv_2005$sqrt_security <- sqrt(iv_2005$total_security) # square root transformation seems to provide something closer, so just use that?

  #hist_plot(iv_2005, sqrt_security, "Histogram of Total Security (Square root)")
  #normality_stats(iv_2005$sqrt_security)


# Ordinal security
iv_2005$ordinal_security <- NA

iv_2005$ordinal_security <- ifelse(between(iv_2005$total_security,0,1), 1, NA)
iv_2005$ordinal_security[iv_2005$total_security == 0] <- 0 
iv_2005$ordinal_security[iv_2005$total_security > 1] <- 2
  # hist_plot(iv_2005, ordinal_security, "Histogram of Ordinal Security")
  # normality_stats(iv_2005$ordinal_security)


# log+1 method
iv_2005$log_security <- log10(iv_2005$total_security+1)
  # hist_plot(iv_2005, log_security, "Histogram of Logged Security")
  # normality_stats(iv_2005$log_security)


# inverse method
iv_2005$inv_security <- 1/(iv_2005$total_security+1)
  # hist_plot(iv_2005, inv_security, "Histogram of inverted Security")



# Box Cox Method
security_boxcox<- geoR::boxcoxfit(iv_2005$total_security, lambda=0, lambda2 = 1) 
  # security_boxcox <- MASS::boxcox(iv_2005$total_security)
```

-   **Stopping rule:**

## Correlated random effects

McNeish et al (2017)

```{stata random effects plot}
xtmixed num_victim_5yr_winz gini_wc gdppc_2004_6_ws age_cent employed male [pw=individual_weight] || country:, pwscale(size)

```

pwcorr rint resids

-   graph of correlation between within-level residuals and random effects

twoway (scatter resids rint) (lfit resids rint)

#### Sampling

> Random digit dialling -- data for Sydney from the national survey. Additional respondents from immigrant and second generation immigrants have been downweighted

> two-stage stratified sampling: administrative region

> of particular groups within the population in the primary samples: -- The 2-stage sampling (random selection of a household and a random selection of a person within that household) means that people from small (single-person) households are by definition over-represented and people from large household are underrepresented. Weight variables are used to compensate for this.

> In each randomly selected household only one randomly selected respondent aged 16 or over was interviewed.

argentina is three-stage

> In each randomly selected household only one randomly selected respondent aged 16 or over was interviewed

> People in households of different sizes have different probabilities of being chosen for the interview, and a weighting procedure is needed to correct this to generate a representative sample of 'persons'.

> household weights that were computed based on gender, household size and regional distribution. The individual weights were computed using the same iterative procedure, but apart from gender and regional distribution, also age and number of adults in the household served as criteria.

from <https://notstatschat.rbind.io/2019/04/19/progress-on-linear-mixed-models-for-surveys/>

from Dr. B on Kenward Roger test:

> Issue is that in smaller samples, sampling distribution may not be normal The KR is an "approximation" method that accounts for small-sample bias Can be used in Stata with dfmethod(kroger) Using this method will also invoke t-distribution The problem is that it can only be used with REML estimation So it really can't be used with survey estimation

More reading needed, but I think I'll be using one of the individual weight items

"individual_weight"\
[10] "individual_weight_for_national_surveys"

"household_weight_n\_ss" "household_country_weight_n\_2000" "indiv_country_weight_n\_2000"

-   Clusters are "primary sampling units" (PSUs)
-   Clustering is sometimes treated as a nuiscance that we try to correct
-   OTHER TIMES, clustering can be an interesting phenomenon to model

Multi-stage sampling Primary sampling units (PSUs) refer to the sampling unit that you start with

Within each PSU (e.g., counties) - Segments o Households  Person

Adjustments - Clusters within PSU are assumed to be representative, so you don't have to make adjustments beyond the PSU

Strata and adjustments - Standard error decreases with strata adjustments - Increases with clustering adjustments - Increases with weighting adjustments

Taylor Series Linearization (TSL) "consists of approximating a complex, nonlinear estimator by a linear function and then computing the variance of the linear approximation" (Valliant 2007:932) Mathematically complex, but essentially results in adjusting standard errors for clustering into strata and PSU's

Book on weighting: applied survey data analysis - Herringa West Berglund

Researchers are resistant to weights because standard errors are often inflated when weights are employed But, the representativeness of one's data will typically be impaired if weights are not employed So, yes, you have to use the \^\$\@&\# weights!

PSU's are often not the end of sampling Surveyors will often sample within PSU's, based on, for example, sets of census blocks (secondary stage sampling) and then households (third stage) The individual within the household then forms the fourth stage of sampling

Heeringa et al (2010:67-68) explain that observations within a PSU form an "ultimate cluster" of observations "The resulting sample can be thought of as a single-stage without replacement selection of ultimate clusters, where all elements within each selected ultimate cluster are sampled" (pg. 68) **Thus, one can consider each PSU as a cluster in which all subsequent aspects of the PSU appropriately represented, so one need only adjust analyses for the sampling based on PSUs**

Sampling without replacement will generally lower variance estimates somewhat You can adjust variance downward using a "finite population correction" But, PSUs are typically assumed to have been sampled with replacement, even though they are typically sampled without replacement The result is simpler variance estimation (because you don't include the FPC) with a slight over-estimation of variance Therefore conservative for population inference

we can't just drop a weight created from a design-based approach into a model-based approach The bigger issue here is that many surveys do not provide information specifically on cluster-level probabilities of selection that would allow us to easily scale our design-based weights

Two approaches that are common are the PWIGLS method 2 and the MPML method A

Chen (2018), a researcher with the Add Health study, argues in a presentation that the PWIGLS method is recommended when informative sampling methods are used in sampling both levels In other words, if one does not have an SRS at both levels

```{r heteroscedasticity, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}

plot(m1_sample, resid(., type = "pearson") ~ fitted(.), abline = 0, main="Heteroscedasticity check - fitted values vs. residuals")

plot(m1_winz, resid(., type = "pearson") ~ fitted(.), abline = 0, main="Heteroscedasticity (Winzorized)")

```

The variance of residuals seems consistent across all levels - so no notable heteroscedasticity?

Not too sure how to interpret these plots - but I think this one isn't great

In particular, the residuals don't appear to be symmetrically distributed (tending to cluster towards the middle of the plot). For low fitted values, residuals seem to be skewed higher, leveling off as the fitted values get larger

> There is a two-parameter version of the Box-Cox transformation that allows a shift before transformation:\The usual Box-Cox transformation sets λ2=0. One common choice with the two-parameter version is λ1=0 and λ2=1 which has the neat property of mapping zero to zero. There is even an R function for this: log1p(). More generally, both parameters can be estimated. In R, the boxcox.fit() function in package geoR will fit the parameters.

> -   Inverse hyperbolic sine (IHS) transformation, mixture models (<https://robjhyndman.com/hyndsight/transformations/>) Other options:
> -   we want to obtain heteroskedasticity robust standard errors and their corresponding t values. In R the function coeftest from the lmtest package can be used in combination with the function vcovHC from the sandwich package to do this.

<https://stackoverflow.com/questions/48479984/r-mixed-model-with-heteroscedastic-data-only-lm-function-works>

> The sandwich package is not limited to lm/glm only but it is in principle object-oriented, see vignette("sandwich-OOP", package = "sandwich") (also published as <doi:10.18637/jss.v016.i09>.
>
> There are suitable methods for a wide variety of packages/models but not for nlme or lme4. The reason is that it's not so obvious for which mixed-effects models the usual sandwich trick actually works. (Disclaimer: But I'm no expert in mixed-effects modeling.)
>
> However, for lme4 there is a relatively new package called merDeriv (<https://CRAN.R-project.org/package=merDeriv>) that supplies estfun and bread methods so that sandwich covariances can be computed for lmer output etc. There is also a working paper associated with that package: <https://arxiv.org/abs/1612.04911>

**Snijders & Bosker, on sandwich estimators (p. 173)** \> sandwich estimators for standard errors, also called cluster-robust standard errors, make no assumptions about the distributional shape of the random coefficients. Verbeke and Lesaffre (1997) and Yan and Bentler (2002) proposed sandwich estimators to provide standard errors applicable for nonnormalily distributed random effects... However, sandwich estimators do require large enough numbers of units at the highest level; see Verbeke and Lesaffre 997), Maas and Hox (2004), and hthe further discussion in 12.2 \* <https://www.r-econometrics.com/methods/hcrobusterrors/>

> For a heteroskedasticity robust F test we perform a Wald test using the waldtest function, which is also contained in the lmtest package. It can be used in a similar way as the anova function, i.e., it uses the output of the restricted and unrestricted model and the robust variance-covariance matrix as argument vcov. Based on the variance-covariance matrix of the unrestriced model we, again, calculate White standard errors.

knief and Forstmeier, 2021 \> Most elegantly,heteroscedasticity can be modeled directly, for instance by using the "weights" argument in lme (see Pinheiro & Bates, 2000, p. 214), which also enables us to test directly whether allowing for heteroscedasticity increases the fit of the model significantly. Similarly, heteroscedasticity-consistent standard errors could be estimated (Hayes & Cai, 2007). For more advice on handling heteroscedasticity, see McGuinness (2002).

> we did not cover collinearity between predictors or the distribution of random effects, but others have dealt with these aspects before (Freckleton, 2011; Schielzeth et al., 2020).

> "Importantly, the robustness of regression methods to deviations from normality of the regression errors e does not only depend on sample size, but also on the distribution of the predictor X (Box & Watson, 1962; Mardia, 1971). Specifically, when the predictor variable X contains a single outlier, then it is possible that the case coincides with an outlier in Y, creating an extreme observation with high leverage on the regression line. This is the only case where statistical significance gets seriously misestimated based on the assumption of Gaussian errors in Y which is violated by the outlier in Y. This problem has been widely recognized (Ali & Sharma, 1996; Box&Watson, 1962; Miller, 1986; Osborne &Waters, 2002; Ramsey & Schafer, 2013; Zuur et al., 2010) leading to the conclusion that Gaussian models are robust as long as there are Trimmed values \> 4\*mean(cook) that occur in X and Y simultaneously."

poisson assumptions \* rate at which events occur is constant \* occurence of one event does not affect occurence of a subsequent event

Probability Mass Function (PMF) - how likely a given value is

cumulative distribution function - all of the likelihoods of values up to and including the designated value

GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA (<https://cehs-research.github.io/eBook_multilevel/gee-count-outcome-epilepsy.html>)

**Snijders & Bosker, on Generalized Estimating Equations (GEE) (p. 1198)**

-   assumes linear or genarlized linear model for the *expected values* of de[emdemt variable in a multilevel data structure, conditional on the explanatory variables

-   No assumptions made with respect to the variances and correlations, except that these are independent between highest-level units

-   linear model parameters estimated through a working model

-   standard errors estimated through sandwich estimator

-   comparison between GEE and HLM discussed in Gardiner et al. 2009

-   LARGE SAMPLE METHOD - without large sample, need a tailored small-sample version with a differently defined sandwich estimators

Match Poisson Regresssion (GLM)

restricted maximum likelihood (REML)

Thus REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values.

lmeresampler provides an easy way to bootstrap nested linear-mixed effects models using either the parametric, residual, cases, CGR (semi-parametric), or random effects block (REB) bootstrap fit using either lme4 or nlme. The output from lmeresampler is an lmeresamp <http://aloy.github.io/lmeresampler/>

bootMer

<https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/> Semi-parametric multilevel modeling (Tihomir Asparouhov)

Probably the advice on this site will be to start with what kind of data your dependent variable is, and then choose an appropriate glm or regression approach. That is, if your dv is count, perhaps Poisson or negative binomial regression. If your dv is concentration, perhaps gamma regression. --

Note that the negative binomial and gamma distributions can only handle positive numbers, and the Poisson distribution can only handle positive whole numbers



```{r, eval=FALSE}
library(brms)
# 4 chains, takes 3-4 days
m_brm <- brms::brm(ordered(num_victim_5yr) ~ age_cent + gdppc_2004_6 + (1|country), data = mod_joined1, family = 'cumulative', prior = set_prior('normal(0, 3)'))
```

```{r brms-model, eval=FALSE}
m100_brms <- brms::brm(ordered(num_victim_5yr_winz) ~ gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed + male + police_effective + income_quartile + (1 | country), family = cumulative,
  data = mod_joined1)
```


