---
title: "ICVS exploration, assumption check, and contingency procedure"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) # pipe
library(purrr) # map function
library(car) #qqPlot
library(survey)
# library(nloptr) # for bobyqa used in svy2lme

library(WeMix)

load("C:/Users/dalla/Google Drive/offline_data_files/icvs_pwt_swiid/data/iv_2005.RData")

source("C:/Users/dalla/Google Drive/project_files/icvs_inequality/scripts/icvs_scripts.r", local = knitr::knit_global())

iv_2005_mod <- iv_2005_mod %>% 
        filter(!is.na(num_victim_5yr))
```

# At a glance

<!--# useful tutorials at https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/ -->

The purpose of this section is to outline the branches of how this data will be explored and modified depending on various assumption checks. This a priori examination is particularly important when analyzing archival data, this data-dependent analysis, the so-called "garden of forking paths [@gelman2014] allows a researcher to pick and choose those data and analyses that best suit their preferred story [@simmons2011].

The following describes two survey sweeps of the International Crime Victimization Survey conducted across 2004-6. At first, this data had `r sweep5_nrow` respondents and `r sweep5_countries` countries. After a exclusion process maximizing the number of countries and observations, there were 31 countries with 85,085 participants, with a minimum of 790 participants per country. An exclusion strategy maximizing the number of variables yields 27 countries, and 48,671 participants.

The following and planned analyses will be carried out with `r sum(summary(iv_2005_mod$country)>0)` countries, and `r format(nrow(iv_2005_mod), big.mark = ",", scientific = FALSE)` participants.

## Software

I would prefer to use an r package for my analyses (lme4, EdSurvey, WeMix, BIFIEsurvey ). However, throughout this screening process, lme4 ran into convergence problems (particularly when incorporating survey weights). Likewise the other packages are so new and specific that they don't generate objects that can be used for analyses like cook's distance.

## Survey Weights

In survey methods, weights are an important tool to make statistics more representative of their populations. Essentially, they increase the weight of individuals who are under-represented in the sample compared to their prevalence in the population. But, depending on the type of analysis we do, survey weights might not be possible.

The use of survey weights has cascading effects: survey weights can only be used with maximum lilkelihood estimation (MLE). However, Restricted Maximum Likelihood Estimation (REML) is preferred for our a small number of level-2 units (28; MLE yields biased estimations of variance, especially under small samples). If we use survey weights, it will impact fundamental decisions such as the type of estimator used.

<!-- If the assumptions underlying a multilevel regression hold in this dataset, the analysis will be conducted using maximum likelihood estimates (MLE) in Stata (IC 14.2; REF), as there are a relatively small number of level-2 units in this data. Likewise, the use of full maximum lilkelihood will enable the use of statistical weights to increase the representativeness of the available data. -->

<!-- Stata is far more developed for multilevel analysis with survey weights. For instance, it provides model-wide Cook's D values for just level-2 units, whereas R can do it only after making a new dataset. If we are going to use a package that has poor integration with rmarkdown, we might as well use the program that best handles the statistics, and save ambitions for a reproducible manuscript for later projects. -->

<!--# The relationship between country-level inequality and the consumption of security products will be tested using a multilevel linear regression. This analysis will be accomplished using a combination of three archival datasets. Firstly, indicators of security consumption have been accessed from the International Crime Victimization survey (ICVS; REF), which is an accumulation of standardized sample surveys to look at householders' experiences with crime, policing, crime prevention and feelings of unsafety. Although it does not consist of longitudinal observations, the ICVS has been distributed across five phases over fifteen years (1989, 1992, 1996, 2000, 2005), totaling 300,000 people and 78 different countries (Data Archiving and Networked Services, n.d.). For the purposes of this study, it is most notable that this survey contains items such as respondents' adopted measures to protect themselves against burglary. Secondly, nation-level inequality in disposable income was accessed through the Standardized World Income Inequality Database (SWIID; REF). The SWIID has been designed to maximize the comparability of income inequality data while maintaining the widest possible coverage across countries and over time. As a result, the gini coefficients are accompanied by standard errors to reflect uncertainty in the estimates. Lastly, countries' expenditure-side real GDP was retrieved from the Penn World Table, version 10.0 (PWT; REF). The selected GDP values are adjusted for Purchasing Power Parity, "to compare relative living standards across countries at a single point in time" (https://www.rug.nl/ggdc/productivity/pwt/?lang=en). Nations' GDP values were divided by their population sizes to yield a per-capita GDP value, which will be used for these analysis.Although the above datasets extend across multiple years, the current analysis will be limited to observations across 2004-2006. Imposing this restriction allows for a sufficient number of countries, combined with relative recency of data. After a conservative exclusion process for missing data, survey sweeps conducted across 2004-6 yielded 31 countries with 85,085 participants, with a minimum of 790 participants per country. An exclusion strategy maximizing the number of variables yields 27 countries, and 48,671 participants. -->

```{r victimization transformation, warning=F, message=FALSE, echo=FALSE}
# Square root method
iv_2005_mod$sqrt_victimization <- sqrt(iv_2005_mod$num_victim_5yr)
  # normality_stats(iv_2005_mod$sqrt_victimization)

# Cube root method
iv_2005_mod$cubrt_victimization <- '^'(iv_2005_mod$num_victim_5yr,1/3) # cube root transformation
  #normality_stats(iv_2005_mod$cubrt_victimization)

# rankit method
iv_2005_mod$rankit_victimization <- RNOmni::RankNorm(iv_2005_mod$num_victim_5yr) # rank-based inverse normal transformation (approximately normalize most distribtutional shapes, and which effectively minimizes type I errors and maximizes statistical power; Bishara & Hittner, 2012; Puth et al., 2014
  #normality_stats(iv_2005_mod$rankit_victimization)

#ggplot(iv_2005_mod, aes(x = rankit_victimization))  +  geom_histogram(binwidth= 1, alpha=.7, fill="#40B0A6", colour='grey') + geom_density(adjust=6, aes(y=..count..)) + theme_minimal()

# Ordinal method
iv_2005_mod$ordinal_victimization <- NA
iv_2005_mod$ordinal_victimization <- ifelse(between(iv_2005_mod$num_victim_5yr,0,1), 1, NA)
iv_2005_mod$ordinal_victimization[iv_2005_mod$num_victim_5yr == 0] <- 0 
iv_2005_mod$ordinal_victimization[iv_2005_mod$num_victim_5yr > 1] <- 2

# Log method
iv_2005_mod$log_victimization <- log10(iv_2005_mod$num_victim_5yr+1)
  #normality_stats(iv_2005_mod$log_victimization)


```

```{r sqrt security, warning=F, message=FALSE, echo=FALSE}
# Square root method
iv_2005_mod$sqrt_security <- sqrt(iv_2005_mod$total_security) # square root transformation seems to provide something closer, so just use that?

  #hist_plot(iv_2005_mod, sqrt_security, "Histogram of Total Security (Square root)")
  #normality_stats(iv_2005_mod$sqrt_security)


# Ordinal security
iv_2005_mod$ordinal_security <- NA

iv_2005_mod$ordinal_security <- ifelse(between(iv_2005_mod$total_security,0,1), 1, NA)
iv_2005_mod$ordinal_security[iv_2005_mod$total_security == 0] <- 0 
iv_2005_mod$ordinal_security[iv_2005_mod$total_security > 1] <- 2
  # hist_plot(iv_2005_mod, ordinal_security, "Histogram of Ordinal Security")
  # normality_stats(iv_2005_mod$ordinal_security)

# log+1 method
iv_2005_mod$log_security <- log10(iv_2005_mod$total_security+1)
  # hist_plot(iv_2005_mod, log_security, "Histogram of Logged Security")
  # normality_stats(iv_2005_mod$log_security)


# inverse method
iv_2005_mod$inv_security <- 1/(iv_2005_mod$total_security+1)
  # hist_plot(iv_2005_mod, inv_security, "Histogram of inverted Security")

# Box Cox Method
security_boxcox<- geoR::boxcoxfit(iv_2005_mod$total_security, lambda=0, lambda2 = 1) 
  # security_boxcox <- MASS::boxcox(iv_2005_mod$total_security)
```

# Exploration

```{r total security, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}
hist_plot(iv_2005_mod, total_security, "Histogram of Total Security")
ggsave(here::here("figures", "security_hist.png"),
       height = 3)

hist_plot(iv_2005_mod, num_victim_5yr_assault, "Histogram of victimizations over past 5 years")
ggsave(here::here("figures", "victim_hist.png"),
       height = 3)

```

```{r security summary}
summary(iv_2005_mod$total_security)

normality_stats(iv_2005_mod$total_security)
```

This summing procedure gives us a max score of `r max(iv_2005_mod$total_security)` on security consumption, with substantial positive skew

```{r lv2 dat, echo=FALSE}
# gd <- iv_2005_mod %>% 
#         filter(!is.na(num_victim_5yr)) %>%
#         group_by(country) %>% 
#         summarise(gdppc_2004_6 = mean(gdppc_2004_6),
#                   gdppc_2004_6_winz = mean(gdppc_2004_6_winz),
#                   gdppc_2004_6_cent = mean(gdppc_2004_6_cent),
#                   num_victim_5yr  = mean(num_victim_5yr),
#                   num_victim_5yr_winz = mean(num_victim_5yr_winz),
#                   gini_2004_6  = mean(av_gini),
#                   gini_winz = mean(gini_2004_6_winz)
#                   )

# gd <- iv_2005_mod %>% 
#         filter(!is.na(num_victim_5yr)) %>%
#         group_by(country) %>% 
#         summarise(gdppc_2004_6 = mean(gdppc_2004_6),
#                   gdppc_2004_6_winz = mean(gdppc_2004_6_winz),
#                   num_victim_5yr  = mean(num_victim_5yr),
#                   num_victim_5yr_winz = mean(num_victim_5yr_winz),
#                   gini_2004_6  = mean(av_gini),
#                   gini_winz = mean(gini_2004_6_winz)
#                   )

gd$country <- as.character(gd$country)
```

```{r descriptives, echo=F}
summary(iv_2005_mod[,c("num_victim_5yr", "age_cent", "employed", "male")])

iv_2005_mod[,vic_var_assault] <- lapply(iv_2005_mod[,vic_var_assault], factor)

summary(iv_2005_mod[,vic_var_assault])

iv_2005_mod[,prevention_min2][is.na(iv_2005_mod[,prevention_min2])] <- 0
iv_2005_mod[,prevention_min2] <- lapply(iv_2005_mod[,prevention_min2], factor)

summary(iv_2005_mod[,prevention_min2])

summary(gd[,c("gini_2004_6", "gdppc_2004_6_cent", "gdppc_2004_6")])
```

## On the distribution

The population distribution for our dependent variables is not known in advance. All we can say is that we know it is a count variable. This uncertainty opens up the possibility of a model selection bias.

The distribution processes we'll consider here are [normal](#assuming-a-normal-distribution), [poisson and negative binomial](#poisson-distribution), and a [three-level logistic regression](#three-level-logistic-regression).

At first glance, victimization looks like a poisson process, but we have limited reason to believe that our outcome variables (e.g., security, number of victimizations) follow a strictly poisson process. Firstly, the count may violate the assumption of independent events; I would guess that purchasing security or being victimized by a crime has *some* effect on later occurrences in either direction (e.g., having less money from the prior security purchase).

Secondly, poisson assumes unbounded counts, where the max value stretches out indefinitely (albeit with smaller and smaller frequency). Our count variables are the result of summing a finite number of binary responses.

One note is that victimizations seem to have inflated zeros. Another is that both variables have larger variances than means - more indicative of negative binomial. A question though - is the independence supposed to be observed within observations or between observations?

All told, let's remember that the assumptions of normality apply to the distribution of residuals, so can only be interpreted in the model. Important to keep skewness of the DV on our radar though.

Starting with scatterplots. Nation Values with a cook's distance of 4 times the mean cook's distance values (for bivariate relationship) are flagged.

## Plotting Influential observations

```{r influence screen, warning=F, message=FALSE, echo=FALSE}


# gd$country <- as.factor(gd$country)

victim_age <- ggplot(iv_2005_mod, aes(x = age_cent, y = num_victim_5yr)) +
    geom_point(alpha = .02, position = "jitter", color = "#40B0A6") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + theme_minimal()+ ggtitle("Age & victimization")


gini_gdp_full <- gd %>% 
  mutate(cooks = cooks.distance(lm(gini_2004_6 ~ gdppc_2004_6)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gini_2004_6, y = gdppc_2004_6, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Gini & GDP - Full") + theme_minimal() + coord_cartesian(xlim = c(20, 65), ylim = c(-10000, 85000)) + ggrepel::geom_label_repel()


# victim GINI plot with outliers
victim_gini_full <- gd %>% 
  mutate(cooks = cooks.distance(lm(num_victim_5yr ~ gini_2004_6)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gini_2004_6, y = num_victim_5yr, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.5) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Victimization & Gini - Full") + theme_minimal() + coord_cartesian(xlim = c(20, 65), ylim = c(.25, 1.125)) + ggrepel::geom_label_repel()


# victim GDP plot with outliers
victim_gdp_full <- gd %>% 
  mutate(cooks = cooks.distance(lm(num_victim_5yr ~ gdppc_2004_6)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gdppc_2004_6, y = num_victim_5yr, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.25) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Victimization & GDP - Full") + theme_minimal() + coord_cartesian(xlim = c(0, 85000), ylim = c(.25, 1.125)) + ggrepel::geom_label_repel(label.size = 0.02)

gridExtra::grid.arrange(victim_age, victim_gini_full,victim_gdp_full, gini_gdp_full, nrow = 2, ncol=2)
```

# Multilevel binomial regression

Initially, a poisson process seems to better describe the distribution of this study's dependent variable. However, the variables *num_victim_5yr* and *security_total* are bounded (i.e., they have true max values), whereas poisson and negative-binomial assume unbounded counts that can extend out indefinitely

@britt2018 provides a summary of the binomial method in the context of counting criminal offences. The dependent variable in this context is the probability of 'success' over multiple trials (in this case, measures).

traditional logistic regression model represents a binomial model with only a single trial (i.e., a Bernoulli

trial)

For instance, in the case of security consumption, the psychological and material impacts of buying a security product (e.g., feelings of safety, reduced budget), can plausibly impact a consumers' decision to purchase further security goods.

# Multilevel ordinal logistic regression

The high degree of skewness, and complete lack of symmetry in the DV distributions, poses some issues for modelling. A good example is out-of sample predictions, since we are using a count variable, negative predicted values are a real concern.

Situations that completely rule out the use of linear methods are: edge effects in the data (i.e. many data points in the highest/lowest category)

When selecting a distribution process, a categorical logistic is obviously more parsimonious. However, a binary or multinomial logistic process in this case misses some meaningful structure to the responses. Firstly, since our DVs are the summed total of binary variables, there is definitely a hierarchy to the values; 1 is larger than 0, and so on.

Secondly, there is likely a meaningful difference between the values of 1, 2, and 3. The argument being that people can stumble into their first unit of security consumption, where they buy a house that happens to have special locks or grates on the window, but those second and third units are more likely to be a deliberate choice to consume security goods.\
- Note: this may be a violation of the proportional odds assumption - "that the relationship between each pair of outcome groups has to be the same" - If this assumption is violated, different models are needed to describe the relationship between each pair of outcome groups. - Namely, we'd expect stronger effects at higher levels of the outcome

## doing ordinal

<https://kevinstadler.github.io/notes/bayesian-ordinal-regression-with-random-effects-using-brms/>

```{r ord}
library(ordinal)

m_ord_null <- clmm(ordered(num_victim_5yr_winz) ~ 1 + (1|country), data = iv_2005_mod)

# country not found
# m_ord <- clmm(ordered(num_victim_5yr) ~ age_cent + gdppc_2004_6, random  = country, data = iv_2005_mod)
          
m_ord <- clmm(ordered(num_victim_5yr_winz) ~ gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed + male + police_effective + income_quartile + (1|country), data = iv_2005_mod)

summary(m_ord)

anova(m_ord_null, m_ord)

mw_ord <- clmm(ordered(num_victim_5yr_winz) ~ gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed + male + police_effective + income_quartile + (1|country), data = iv_2005_mod, weights = individual_weight)


# https://user2021.r-project.org/participation/technical_notes/t186/technote/
# vizualization requires clmm2, specify Hess = TRUE to get summary

# Warning: clmm2 may not have converged:
#  optimizer 'ucminf' terminated with max|gradient|: 0.0442298313166208
m_ord2 <- clmm2(ordered(num_victim_5yr) ~ gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed + male + police_effective + income_quartile, random = country, data = iv_2005_mod, Hess = TRUE)

summary(m_ord2)
```

## Ordinal with all SWIID dataframes

```{r SWIID_100 prep}
load("C:/Users/dalla/Google Drive/offline_data_files/icvs_pwt_swiid/data/swiid9_0.rda")

unique(swiid[[1]]$country)

swiid_responders <-  swiid %>%
  map(. %>% filter(.,country %in% names(summary(iv_2005_mod$country)[0!= summary(iv_2005_mod$country)]) )) %>%
  map(. %>% select(country,year,gini_disp))


unique(swiid_responders[[1]]$country)

swiid_responders[[1]]
```

```{r PWT_100}
pwt <- readxl::read_excel("C:/Users/dalla/Google Drive/offline_data_files/icvs_pwt_swiid/data/pwt100.xlsx", sheet = "Data")

# changed from rgdpe
pwt100_gdppc <- pwt %>% 
  transmute(country = country,
            year = year,
            gdppc = cgdpe/pop) %>%
  filter(!is.na(gdppc))

pwt_responders <-  pwt100_gdppc %>%
  filter(country%in% names(summary(iv_2005_mod$country)[0!= summary(iv_2005_mod$country)]) )
```

```{r merge icvs and PWT}
pwt_spread <- spread(pwt_responders, year,gdppc) 

colnames(pwt_spread)[2:6] = c("gdppc_2003", "gdppc_2004", "gdppc_2005", "gdppc_2006", "gdppc_2007") 

pwt_2004_6 <- pwt_spread %>%
  dplyr::rowwise() %>%
  dplyr::mutate(gdppc_2004_6 = mean(c(gdppc_2004, gdppc_2005, gdppc_2006)))
```


## other packages

GLMMadaptive brms MCMC

MASS::polr() (just fixed effect?) mixor (package 'mixor' is not available for this version of R)

Different statistical packages support different link families, for example the ordinal package (which offers ordinal regression with one random effect) supports the cumulative links "logit", "probit", "cloglog", "loglog" and "cauchit"

cumulative link models



```{r GLMMadaptive}
library(GLMMadaptive)

cr_vals <- cr_setup(iv_2005_mod$num_victim_5yr_winz)

cr_data <- iv_2005_mod[cr_vals$subs, ]
cr_data$y_new <- cr_vals$y
cr_data$cohort <- cr_vals$cohort

fm <- mixed_model(y_new ~ cohort + gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed + male + police_effective + income_quartile, random = ~ 1 | country, 
                  data = cr_data, family = binomial())

summary(fm)
```

```{r brms}
library(brms)
# 4 chains, takes 3-4 days
m_brm <- brms::brm(ordered(num_victim_5yr) ~ age_cent + gdppc_2004_6 + (1|country), data = iv_2005_mod, family = 'cumulative', prior = set_prior('normal(0, 3)'))
```

rdered logistic (or more accurately ordered logit) regression is an extension of logistic/logit regression: where in logistic regression you model one coefficient that captures the relative likelihood (in log-odds) of one outcome occurring over another (i.e. 2 outcomes captured by 1 coefficient), ordered logit regression models the relative likelihood of k different outcomes based on k-1 coefficients. The k-1 coefficients in the model capture the cumulative likelihood of responses falling into an expanding set of ordered response categories, e.g. in the case of 4 possible outcome categories (o1-o4) the three intercepts of the model, each of which will be at least as high as the preceding one, capture the baseline log-odds of observing:

log(o1 vs. o2, o3, o4) log(o1, o2 vs. o3, o4) log(o1, o2, o3 vs. o4)

The choice of link function is typically not critical and most methods assume the "logit" function (the log-odds transformation that forms the basis of ordered logit regression) by default, but a different choice can be informed by your knowledge of the data.

```{r}
# testing parallel regression assumption using Brant's test
brant(m_ord)
```

## doing bayesian

"All Bayesian models were created in Stan computational framework (<http://mc-stan.org/>) accessed with brms package (Bürkner, 2017). To improve convergence and guard against overfitting, we specified mildly informative conservative priors."

Solid red points show fitted values: the mean of posterior distribution and 95% CI

```{r brm2}
# model specification
mod = brm(real ~ corpus + (1|sound) + (1|id), data = df, family = 'bernoulli', prior = set_prior('normal(0, 3)'),
iter = 1000, chains = 4, cores = 4)
# model inspection
summary(mod)
plot(mod)
pp_check(mod)
# get fitted values
newdata = data.frame(corpus = levels(df$corpus))
fit = fitted(mod, newdata = newdata, re_formula = NA) * 100
colnames(fit) = c('fit', 'se', 'lwr', 'upr')
df_plot = cbind(newdata, fit) # and plot with ggplot
# get contrasts
fit1 = as.data.frame(fitted(mod, newdata = newdata, re_formula = NA, summary = FALSE))
colnames(fit1) = newdata$corpus
ut_vs_rest = fit1$ut - (fit1$belin + fit1$cordaro + fit1$hawk + fit1$lima + fit1$maurage + fit1$simon) / 6
quantile(ut_vs_rest, probs = c(.5, .025, .975))
hawk_vs_rest = fit1$hawk - (fit1$belin + fit1$cordaro + fit1$lima + fit1$maurage + fit1$simon) / 5
quantile(hawk_vs_rest, probs = c(.5, .025, .975))
ut_vs_hawk = fit1$ut - fit1$hawk
quantile(ut_vs_hawk, probs = c(.5, .025, .975))
```

# questions

What is MCMC? ● What is a prior and why should I use one? ● Does my model make sense? (convergence, posterior prediction) ● Fitted values (brms::fitted.brmsfit) vs. posterior prediction (brms::predict.brmsfit) ● Shrinkage (e.g. brms::horseshoe) ● How can I extract and customize STAN code (brms::stancode)?

```{r MCMCglmm}
m1<-MCMCglmm(cbind(tarsus, back) ~ trait:sex + trait:hatchdate - 1, random = ~ us(trait):animal + us(trait):fosternest, rcov = ~ us(trait):units, prior = prior, family = rep("gaussian", 2), nitt = 60000, burnin = 10000, thin=25, data = BTdata, pedigree=BTped)
```

## assumptions

<https://medium.com/evangelinelee/ordinal-logistic-regression-on-world-happiness-report-221372709095>

    The dependent variable are ordered.
    One or more of the independent variables are either continuous, categorical or ordinal.
    No multi-collinearity.
    Proportional odds



    Now we should conduct the Brant Test to test the last assumption about proportional odds. This assumption basically means that the relationship between each pair of outcome groups has to be the same. If the relationship between all pairs of groups is the same, then there is only one set of coefficient, which means that there is only one model. If this assumption is violated, different models are needed to describe the relationship between each pair of outcome groups.

# Assuming a normal distribution {#assuming-a-normal-distribution}

The case for using a normal distribution:

@knief2021 found a Gaussian error distribution to be far more robust to violations of its assumptions than Poisson and Binomial distributions. Given the potential for heavy bias in poisson and negative binomial models, a gaussian error structure's comparative robustness appears to make a (violated) assumption of normality be the safer choice compared to a possibly misattributed poisson distribution.

> We have shown that Poisson models yielded heavily biased type I error rates (at α = 0.05) in either direction ranging from 0 to as high as 0.55 when their distribution assumption is violated (Fig. 3 right column, Fig. S7).

@knief2021 also concluded that for anything other than very small sample sizes:

> "p-values from Gaussian models are highly robust to even extreme violation of the normality assumption and can be trusted, except when involving X and Y distributions with extreme outliers (p. ??). Likewise, "for N = 1000, power was essentially unaffected by the distribution of Y and X."

For our purposes, it is important to note that we are dealing with a small sample size at the cluster/nation level.

## Assumption checks and and contingencies

### Influential observations

-   **Check:** cook's D values exceeding the threshold 4/n; where n = \# of clusters.
-   **Treatment:** winzorize variables that demonstrate exhibit sufficient influence on model
-   **Robustness check:**

a.  Retain the level-1 cases of extreme values, add a dummy variable at level-2 to control for their unit, rerun analysis (Van de Meer et al, 2010). Document whether any coefficients change.

b.  Run analyses excluding cases that exceed the threshold 4/n; where n = \# of clusters. Following, evaluate cook's D values on this reduced dataset.

c.  Eliminate any cases that exceed a cook's d of .5, and conduct a third analysis.

*Then document whether any coefficients have gained/lost significance*

### Linearity

-   **Check:** Scatterplots for nonlinear relationships
-   **Treatment:** If robust nonlinear relationship detected, add polynomials until adequate fit to linear function is obtained

Nonlinearity does not seem to be a major issue in the data. There may be some pattern of as much as a third-order polynomial in the effects of gini on victimization, (second-order for GDP).

Especially with a small sample at level-2, adding polynomials seems more likely to risk overfitting the data. I would only assess the fit of a polynomial model if I could then test it on another dataset, for cross-validation.

### Normality in residuals

-   **Check**: QQplots (The Normal Probability Plot method.) for whether lie on normal distribution

**NO treatment:** Although there are many possible transformations possible for this kind of data (square root, rankit/INT, log(x+1), boxcox(x+1)), nonnormal data, by itself, is not expected to adversely impact the proposed analysis. Lumley, Diehr, Emerson, & Chen (2002) provided simulation evidence that linear regressions conducted on large samples are robust to departures from normality. With at least 500 observations, they found that linear regression can be conducted with negligible impact on Type I errors.

### Heteroscedasticity (Homogeneity of Variance)

-   **Check**: Plot residuals vs predicted values, for changes in residual variance across levels of predicted values

-   **Treatment:** Transformation of DV (e.g., box-cox, square root, log, rankit). Beyond this, the model already has robust standard errors

> There is a two-parameter version of the Box-Cox transformation that allows a shift before transformation:\The usual Box-Cox transformation sets λ2=0. One common choice with the two-parameter version is λ1=0 and λ2=1 which has the neat property of mapping zero to zero. There is even an R function for this: log1p(). More generally, both parameters can be estimated. In R, the boxcox.fit() function in package geoR will fit the parameters.

<!-- Inverse hyperbolic sine (IHS) transformation, mixture models (<https://robjhyndman.com/hyndsight/transformations/>) Other options: -->

> For a heteroskedasticity robust F test we perform a Wald test using the waldtest function, which is also contained in the lmtest package. It can be used in a similar way as the anova function, i.e., it uses the output of the restricted and unrestricted model and the robust variance-covariance matrix as argument vcov. Based on the variance-covariance matrix of the unrestriced model we, again, calculate White standard errors.

Wald test: a way to find out if explanatory variables in a model are significant.

If the Wald test shows that the parameters for certain explanatory variables are zero, you can remove the variables from the model

Weighted distance between the unrestricted estimate and its hypothesized value under the null hypothesis, where the weight is the precision of the estimate

<!-- >knief2021  Most elegantly,heteroscedasticity can be modeled directly, for instance by using the "weights" argument in lme (see Pinheiro & Bates, 2000, p. 214), which also enables us to test directly whether allowing for heteroscedasticity increases the fit of the model significantly. Similarly, heteroscedasticity-consistent standard errors could be estimated (Hayes & Cai, 2007). For more advice on handling heteroscedasticity, see McGuinness (2002). -->

### Multicolinnearity

-   **Check**: VIF/tolerances
-   **Fix** remove values \>= 5

> we did not cover collinearity between predictors or the distribution of random effects, but others have dealt with these aspects before (Freckleton, 2011; Schielzeth et al., 2020).

## Model-based exploration

Assuming a normal distribution, there are likely some heavy tails in the residuals.

```{r estimation preview2, warning=T, message=FALSE}
# Fixed effect variables in model

m1_null <- lme4::lmer(num_victim_5yr ~ (1 | country), data = iv_2005_mod, REML=FALSE)

# Estimate model
m1_sample <- lme4::lmer(num_victim_5yr ~ gini_2004_6_cent + gdppc_2004_6_scale + 
                     age_cent + employed + male + police_effective + income_quartile + (1 | country), data = iv_2005_mod, REML=FALSE)

#row-maximizing model
# m1_sample <- lme4::lmer(num_victim_5yr ~ gini_2004_6_cent + gdppc_2004_6_scale +
#                     age_cent + employed + male + police_effective + income_quartile + (1 | country), data = iv_2005_mod, REML=FALSE)

# call basic unweighted model
summary(m1_sample)

# estimate weighted model - fails to converge (even with all numeric variables scaled)
m1_weight <- lme4::lmer(num_victim_5yr ~ gini_2004_6_cent + gdppc_2004_6_scale + age_cent + employed +   male + (1 | country),data = iv_2005_mod, REML=FALSE, weights = individual_weight)
```

Further seen in the normality statistics for the residuals - high skewness and kurtosis

```{r model skewness kurtosis,  warning=F, message=FALSE}
moments::skewness(resid(m1_sample))
moments::kurtosis(resid(m1_sample))
```

Indeed, this model's residuals are definitely not normally-distributed, with a skewness of `r round(moments::skewness(resid(m1_sample)),2)`, and a kurtosis of `r round(moments::kurtosis(resid(m1_sample)),2)`. However, this is not yet the time to examine the residual plots - we need to explore and treat any potential influential observations that might be exacerbating these violations of normality.

<!-- > Note that this is based on simple aggregates. It is not based on empirical Bayesian estimates Doesn't take sampling variation due to sampling of into account Therefore a rough guide, but only rough -->

```{r VIF1, warning=F, message=FALSE}
performance::check_collinearity(m1_sample)

car::vif(m1_sample) # display VIF values
```

Multicollinearity seems okay

### Cook's distance ( R package influence.ME )

**Other influence stats**

DFBeta values: difference between estimate including and deleting outlier

DFFit: difference between adjusted predicted value and original value

```{r cooksd model, warning=F, message=FALSE, fig.show="hold", out.width="50%"}
# influential cases (Cook's Distance. Any values over 1 are likely to be significant outliers)
cooksD <- cooks.distance(m1_sample)
summary(cooksD)
```

```{r cooksd lv2, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}
# look for influential observations in nation-level
plot(cooksD, main="Model-wide Cook's D * BEWARE Y AXIS", ylim=c(0,.01))
abline(h = 4/nrow(iv_2005_mod), lty = 2, col = "steelblue") # add cutoff line
abline(h = 4*mean(cooksD, na.rm=T), col="red") 


victim_nation_cook <- cooks.distance(lm(num_victim_5yr ~ gini_2004_6 + gdppc_2004_6, gd))

plot(victim_nation_cook, main="Cook's D, Victimization ~ Gini + GDP", ylim=c(0,1))
abline(h = 4/nrow(gd), lty = 2, col = "steelblue") # add cutoff line
abline(h = 4*mean(victim_nation_cook, na.rm=T), col="red") 
text(x=1:length(victim_nation_cook)+1, y=victim_nation_cook, labels=ifelse(victim_nation_cook>4*mean(victim_nation_cook, na.rm=T), gd$country,""), col="red") 

```

A model-wide Cook's D shows that any given observation has a small influence on the entire model - to be expected with such a large sample. This is why it's important to break the analysis of influential variables into multiple levels

We can see that level-2 analysis yields much larger Cook's D values. The cooks distance values are all well under 1. Importantly, this is the same for the nation-level variables; they have a smaller sample size - and are thus the more susceptible to influential observations.

**However, there are a few observations at nation-level that exceed the 4/n threshold, and some that exceed 0.5. Generally, there are changes in the slope, but ultimately the direction doesn't change**

## Handling influential observations - Winzorization

Going on with citing @knief2021:

> "Importantly, the robustness of regression methods to deviations from normality of the regression errors e does not only depend on sample size, but also on the distribution of the predictor X (Box & Watson, 1962; Mardia, 1971). Specifically, when the predictor variable X contains a single outlier, then it is possible that the case coincides with an outlier in Y, creating an extreme observation with high leverage on the regression line. This is the only case where statistical significance gets seriously misestimated based on the assumption of Gaussian errors in Y which is violated by the outlier in Y. This problem has been widely recognized (Ali & Sharma, 1996; Box&Watson, 1962; Miller, 1986; Osborne &Waters, 2002; Ramsey & Schafer, 2013; Zuur et al., 2010) leading to the conclusion that Gaussian models are robust as long as there are Trimmed values \> 4\*mean(cook) that occur in X and Y simultaneously."

Since these influential observations are not a mistake - but rather truly extreme data, we will not delete the data, but instead winzorize these extreme values.

```{r winzorized descriptives, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}
iv_2005_mod[,c("num_victim_5yr", "num_victim_5yr_winz")] %>%
  summary()

gd[,c("gdppc_2004_6", "gdppc_2004_6_winz", "gini_2004_6", "gini_winz")] %>%
  summary()
```

Dramatic changes to the max values, less-so for the mean values (with the exception of number of victimizations)

```{r winzorize histograms, warning=F, message=FALSE, echo=FALSE}

gridExtra::grid.arrange(


hist_plot(iv_2005_mod,num_victim_5yr,"Regular number of victimizations"),
hist_plot(iv_2005_mod,num_victim_5yr_winz,"Winzorized number of victimizations"),


hist_plot(gd,gdppc_2004_6,"Regular GDP"),
hist_plot(gd,gdppc_2004_6_winz,"Winzorized GDP"),

hist_plot(gd,gini_2004_6,"Regular Gini"),
hist_plot(gd,gini_winz,"Winzorized Gini"),

nrow = 3, ncol=2)

```

### Winzorized plots

```{r winz scatter, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}
gini_gdp_winz <- gd %>% 
  mutate(cooks = cooks.distance(lm( gdppc_2004_6_winz ~ gini_winz)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gini_winz, y = gdppc_2004_6_winz, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.6) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Gini & GDP - Winzorized") + theme_minimal() + coord_cartesian(xlim = c(20, 65), ylim = c(-10000, 85000)) + ggrepel::geom_label_repel()

# victim GINI plot, winzorized
victim_gini_winz <- gd %>% 
  mutate(cooks = cooks.distance(lm(num_victim_5yr_winz ~ gini_winz)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gini_winz, y = num_victim_5yr_winz, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.6) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Victimization & Gini - Winzorized") + theme_minimal() + coord_cartesian(xlim = c(20, 65), ylim = c(.25, 1.125)) + ggrepel::geom_label_repel()

# victim GDP plot, winzorized
victim_gdp_winz <- gd %>% 
  mutate(cooks = cooks.distance(lm(num_victim_5yr_winz ~ gdppc_2004_6_winz)),
         outlier_country = ifelse(cooks > 4*mean(cooks), country, NA)) %>%
  ggplot(aes(x = gdppc_2004_6_winz, y = num_victim_5yr_winz, label = outlier_country)) +
  geom_point(position = "jitter") + geom_smooth(alpha = .25, color = "red", span = 1.6) + geom_smooth(method = "lm", alpha = .25) + ggtitle("Victimization & GDP - Winzorized") + theme_minimal() + coord_cartesian(xlim = c(0, 85000), ylim = c(.25, 1.125)) + ggrepel::geom_label_repel(label.size = 0.02)



victim_gini_full
victim_gini_winz
victim_gdp_full
victim_gdp_winz
# gini_gdp_full
# gini_gdp_winz
```

```{r sqrt winz, echo = FALSE, warning=FALSE}
iv_2005_mod <- iv_2005_mod %>%
  filter(!is.na(num_victim_5yr)) %>%
  mutate(num_victim_5yr_winz_sqrt = sqrt(num_victim_5yr_winz),
          gdppc_2004_6_winz_sqrt = sqrt(gdppc_2004_6_winz),
          gini_2004_6_winz_sqrt = sqrt(gini_2004_6_winz),
         num_victim_5yr_winz_rankit = RNOmni::RankNorm(num_victim_5yr+1)
         )

m1_winz_sqrt <- lme4::lmer(num_victim_5yr_winz_sqrt ~ gdppc_2004_6_winz_sqrt + gini_2004_6_winz_sqrt +
                     age_cent + employed + male + police_effective + income_quartile + (1 | country),data = iv_2005_mod, REML=FALSE)
```

```{r winzorize model, message=F, warning=FALSE}
m1_winz <- lme4::lmer(num_victim_5yr_winz ~ gdppc_2004_6_wc + gini_2004_6_winz +
                     age_cent + employed + male + police_effective + income_quartile + (1 | country),data = iv_2005_mod, REML=FALSE)
```

```{r model winz skewness kurtosis,  warning=F, message=FALSE}
moments::skewness(resid(m1_winz))
moments::kurtosis(resid(m1_winz))
```

Winzorizing reduces non-normality, from a skewness of `r round(moments::skewness(resid(m1_sample)),2)` to skewness of *`r round(moments::skewness(resid(m1_winz)),2)`*, and from a kurtosis of `r round(moments::kurtosis(resid(m1_sample)),2)` to *`r round(moments::kurtosis(resid(m1_winz)),2)`*

QQplots likewise suggest the reduction (but not resolution) of nonnormal error distributions

```{r Normality, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"}
qqnorm(resid(m1_sample))
qqline(resid(m1_sample), col = "darkgreen")

plot(fitted(m1_sample),resid(m1_sample), main = "Normal resid ~ fitted")


qqnorm(resid(m1_winz), main = "Winzorized gini + victim - QQ plot")
qqline(resid(m1_winz), col = "darkgreen")

plot(fitted(m1_winz),resid(m1_winz), main = "Winzorized resid ~ fitted")


# qqnorm(resid(m1_winz_sqrt), main = "sqrt winzorized gini + victim - QQ plot")
# qqline(resid(m1_winz_sqrt), col = "darkgreen")

```

Winzorizing definitely seems to help the normality of the model. Again, it does not remedy normality by any stretch; there are still some very heavy tails

Likewise, there is some imbalance in the resid\~fitted plots, with more positive residuals accompanying smaller fitted values.

The variance of residuals seems consistent across all levels - so no notable heteroscedasticity?

In particular, the residuals don't appear to be symmetrically distributed (tending to cluster towards the middle of the plot). For low fitted values, residuals seem to be skewed higher, leveling off as the fitted values get larger

Analyses on archival data provides many degrees of freedom, not just in the processing of data, but in model construction as well.

1.  run many clusters
2.  run few clusters - few variables

iv_2005

xtmixed total_security num_victim_5yr gini_cent gdppc_2004_6\_scale age_cent employed male [pw=individual_weight] \|\| country:, pwscale(size)

3.  few clusters - many variables
4.  few clusters - many variables - winzorized **Report this one**
5.  few clusters - many variables - flagged
6.  many clusters - winzorized

xtmixed security_winz num_victim_5yr_winz gini_wc gdppc_2004_6\_ws age_cent employed male [pw=individual_weight] \|\| country:, pwscale(size)

7.  many clusters - flagged
8.  Many clusters - with national victimization
9.  few clusters- with

# Poisson distribution {#poisson-distribution}

Poisson is the go-to distribution for handling count data, which tends to exhibit a positive skew, and could provide an alternate way to model our data.

<!-- poisson assumptions \* rate at which events occur is constant \* occurence of one event does not affect occurence of a subsequent event -->

<!-- Probability Mass Function (PMF) - how likely a given value is -->

<!-- cumulative distribution function - all of the likelihoods of values up to and including the designated value -->

```{r pois dis, message=FALSE}
library('fitdistrplus')
plot(fitdist(iv_2005_mod$total_security,"pois"))
plot(fitdist(iv_2005_mod$num_victim_5yr,"pois"))


# ggplot(data = iv_2005_mod,
#        mapping = aes(sample = total_security)) + 
#   stat_qq(distribution = stats::qpois,
#           dparams = list(lambda = mean(iv_2005_mod$total_security))) + 
#   geom_step(data = data.frame(x = 0:10,
#                               total_security = 0:10),
#             mapping = aes(x = x,
#                           y = total_security),
#             colour = "red",
#             alpha = 0.5) 

# print("Security mean is")
mean(iv_2005_mod$total_security)
# print("Security variance is")
var(iv_2005_mod$total_security)

# print("Victimization mean is")
mean(iv_2005_mod$num_victim_5yr)
# print("Victimization variance is")
var(iv_2005_mod$num_victim_5yr)
```

Mean and variance values are slightly overdispersed (larger variances than means) - How close do they need to be to argue for poisson?

Specifying the poisson model:

```{r poisson model, message=FALSE, fig.show="hold", out.width="50%"}
m_pois <- glmer(num_victim_5yr ~ gdppc_2004_6_scale + gini_2004_6_cent + age_cent + employed + male + police_effective + income_quartile + (1 | country), family = poisson, data = iv_2005_mod)
```

## Poisson assumptions and contingencies

The traditional procedure for testing binomial assumptions is conducted in seven steps:

1\. Conduct the D&L score test for overdispersion (H0: Poisson vs. H1: NB).

2\. If the D&L score test fails to reject the null, conduct a Vuong test for zero-inflation (H0: Poisson vs. H1: ZIP). *Otherwise, proceed to Step 5*.

3\. If the Vuong test for zero-inflation fails to reject the null, **fit a Poisson GLM** and calculate the p-value (H0 : βX = 0 vs. H1 : βX ≠ 0). *Otherwise, proceed to Step 4.*

4\. If the Vuong test for zero-inflation rejects the null, **fit the ZIP model** and calculate the p-value (H0 : βX = γX = 0).

5\. If the D&L score test rejects the null, conduct the Vuong test for zero-inflation (H0: NB vs. H1: ZINB).

6\. If the Vuong test for zero-inflation fails to reject the null, **fit the NB model** and calculate the p-value (H0 : βX = 0). *Otherwise, proceed to Step 7.*

7\. If the Vuong test for zero-inflation rejects the null, **fit the ZINB regression model** and calculate the p-value (H0 : βX = γX = 0).

Following a simulation analysis of this seven-step testing procedure @campbell2021 discusses the consequences of testing the assumptions of a poisson distribution:

> if one does not have sufficient power to confidently test for overdispersion and zero-inflation, it may be best to simply use a model that can accommodate for these possibilities (e.g. use a robust model) instead of going through a model selection procedure that might inflate the type 1 error.

> However, when the distributional assumptions of a Poisson GLM do hold, Tsou (2006) acknowledge that the 'robust approach might not be as efficient'.

This loss of efficiency reduces statistical power, so researchers may seek other methods to correct for model selection bias

> Our simulation study suggests that, if sample sizes are sufficiently large, there is little need to worry about model selection bias following a series of sequential score tests. However, when sample sizes are small, our simulation study demonstrated that model selection bias can lead to potentially substantial type 1 error inflation

The assumptions for poisson (and negative binomial) models will be tested using the r package **DHARMa: residual diagnostics for hierarchical (multi-level/mixed) regression models**

> The 'DHARMa' package uses a simulation-based approach to create readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed models.

<!-- Currently supported are linear and generalized linear (mixed) models from ‘lme4’ (classes ‘lmerMod’, ‘glmerMod’), ‘glmmTMB’, ‘GLMMadaptive’ and ‘spaMM’, generalized additive models (‘gam’ from ‘mgcv’), ‘glm’ (including ‘negbin’ from ‘MASS’, but excluding quasi-distributions) and ‘lm’ model classes -->

> To interpret the residuals, remember that a scaled residual value of 0.5 means that half of the simulated data are higher than the observed value, and half of them lower. A value of 0.99 would mean that nearly all simulated data are lower than the observed value.

```{r dharma}
library(DHARMa) #  “Diagnostics for HierArchical Regression Models”
simulationOutput <- simulateResiduals(fittedModel = m_pois, plot = F)
```

### Overdispersion

As said above, overdispersion describes excess variance, given the mean.

\- **Check:** DHARMA:: testDispersion()

\- **Treatment:** negative binomial model

\- **Robustness check:**

```{r dharma dispersion}
testDispersion(simulationOutput)

# another package, blemco

blmeco::dispersion_glmer(m_pois)  # chec for dispersion, cutoff 1.4 https://stackoverflow.com/questions/22842017/model-checking-and-test-of-overdispersion-for-glmer
# # cutoff 1.1 https://stats.stackexchange.com/questions/230721/how-to-check-overdispersion-of-binomial-glmms-lme4-package
```

### Zero-inflation

Zero-inflation describes the extent to which more zeros are present in the data than what the distribution assumes

-   **Check:** DHARMA:: testZeroInflation()
-   **Treatment:** zero-inflated model (glmmTMB)

```{r dharma zero inflation}
testZeroInflation(simulationOutput)
```

Simulation outliers (data points that are outside the range of simulated values) are highlighted as red stars. To provide a visual aid in detecting deviations from uniformity in y-direction, the plot function calculates an (optional default) quantile regression, which compares the empirical 0.25, 0.5 and 0.75 quantiles in y direction (red solid lines) with the theoretical 0.25, 0.5 and 0.75 quantiles (dashed black line), and provides a p-value for the deviation from the expected quantile.

<!--# http://glmmadmb.r-forge.r-project.org/glmmADMB.html -->

glmmADMB: hurdle models

> In contrast to zero-inflated models, hurdle models treat zero-count and non-zero outcomes as two completely separate categories, rather than treating the zero-count outcomes as a mixture of structural and sampling zeros. he two parts of the model have to be fitted separately, however. First we fit a truncated distribution to the non-zero outcomes...Then we fit a model to the binary part of the data (zero vs. non-zero).

```{r dharma outliers}
testOutliers(simulationOutput)
```

By default, plotResiduals plots against predicted values. However, you can also use it to plot residuals against a specific other predictors (highly recommend).

```{r dharma plot against}
testQuantiles(simulationOutput)

plotResiduals(simulationOutput, form = iv_2005_mod$gini_2004_6_cent)
```

```{r poisson qq, echo = FALSE}
# qqnorm(resid(m_pois), main = "Poisson - QQ plot")
# qqline(resid(m_pois), col = "darkgreen")
# plot(fitted(m_pois),resid(m_pois), main = "Poisson resid ~ fitted")

# Likewise qq plot indicates the bottom quantiles deviate from the normality line. Not sure whether it's anticipated, but some indication of heteroscedasticiy in the poisson model, with variance decreasing with larger fitted values
```

```{r dharma plots}
plot(simulationOutput)
```

```{r poisson overdispersion, message=FALSE}
blmeco::dispersion_glmer(m_pois)  # chec for dispersion, cutoff 1.4 https://stackoverflow.com/questions/22842017/model-checking-and-test-of-overdispersion-for-glmer
# # cutoff 1.1 https://stats.stackexchange.com/questions/230721/how-to-check-overdispersion-of-binomial-glmms-lme4-package
# 
# # zero inflation test https://stats.stackexchange.com/questions/118322/how-to-test-for-zero-inflation-in-a-dataset
# vcdExtra::zero.test(iv_2005_mod$total_security)
```

## Other packages to run modified glmm

<!-- https://stats.stackexchange.com/questions/474228/zero-inflated-generalized-poisson-mixed-effect-model-with-glmmtmb-still-zero-inf  -->

glmmTMB package with a Zero-inflated generalized Poisson

<!-- https://drizopoulos.github.io/GLMMadaptive/ -->

GLMMadaptive: fits mixed effects models for grouped/clustered outcome variables

**Vuong test**: a test of distinguishability and fit for two models compares the zero-inflated model with an ordinary Poisson regression model

```{r vuong, echo=FALSE}
# https://stackoverflow.com/questions/51016328/mixed-effects-model-with-zero-inflated-data-error-message-using-zeroinfl-fro 
# m_zip <- glmmTMB::glmmTMB(num_victim_5yr ~ gdppc_2004_6_scale  + av_gini + age_cent + employed + male + police_effective + income_quartile + (1 | country),zi = ~ gdppc_2004_6_scale  + av_gini + age_cent + employed + male, data = iv_2005_mod, family = poisson)


# 
# summary(m_zip)

# If you set zi ~ 1 then the ZIM is assumed to be independent of any predictors. 

# Interpreting zero-inflation coefficients: every unit change in predictor increases/decreases the log odds of the being in the structural zero group (those who cannot have cancer) 

# mnull <- update(m_zip)
# 
# pchisq(2 * (logLik(m_zip) - logLik(mnull)), df = 3, lower.tail = FALSE)
# # 
# p1 <- glm(count ~ child + camper, family = poisson, data = zinb)
# 
# pscl::vuong(m1,m_pois)
```

```{r nbinom, echo=FALSE}

# m_bino <- glmer.nb(num_victim_5yr ~ gdppc_2004_6_scale + gini_2004_6_cent + age_cent + employed + male + police_effective + income_quartile + (1 | country),data = iv_2005_mod)
# 
# bino_sim <- simulateResiduals(fittedModel = m_bino, plot = F)

# testDispersion(m_bino)
# testDispersion(bino_sim)
# testZeroInflation(bino_sim)

# qqnorm(resid(m_bino), main = "negative binomial - QQ plot")
# qqline(resid(m_bino), col = "darkgreen")
# plot(fitted(m_bino),resid(m_bino), main = "negative binomial resid ~ fitted")


#compares Zero-inflated Poisson regression
# pscl::vuong(poisson,zero inflat pois)

```

```{r other distributions, warning=F, message=FALSE, echo=FALSE, results='hide', fig.show="hold", out.width="50%"}
# https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/

# # unexpected missingness exclusion
# summary(!is.na(iv_2005_mod[["num_victim_5yr"]]))
# which(is.na(iv_2005_mod[["num_victim_5yr"]]))
# 
# iv_2005_mod$num_victim_5yr_winz_1 <- iv_2005_mod$num_victim_5yr_winz + 1
# 
# norm <- MASS::fitdistr(iv_2005_mod$num_victim_5yr_winz_1, "normal")
# 
# # qqp(iv_2005_mod$num_victim_5yr, "norm", main = "Normal distribution")
# 
# qqp(iv_2005_mod$num_victim_5yr_winz_1, "norm", main = "Normal distribution")
# 
# 
# qqp(sqrt(iv_2005_mod$num_victim_5yr_winz), "norm", main = "Normal distribution, square root")
# # qqp(sqrt(iv_2005_mod$num_victim_5yr_winz), "norm", main = "winzorized and square root")
# 
# qqp(iv_2005_mod$num_victim_5yr_winz_1, "lnorm", main = "Logged normal distribution")
# # qqp(iv_2005_mod$num_victim_5yr_winz + 1, "lnorm", main = "Winzorized and Logged")
# 
# # negative binomial - fails to optimize with +1 transformation
# # nbinom <- MASS::fitdistr(iv_2005_mod$num_victim_5yr_1, "Negative Binomial")
# # qqp(iv_2005_mod$num_victim_5yr_1, "nbinom", main = "Negative binomial distribution", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])
# 
# 
# #poisson distribution
# poisson <- MASS::fitdistr(iv_2005_mod$num_victim_5yr_winz_1, "Poisson")
# qqp(iv_2005_mod$num_victim_5yr_winz_1, "pois", main = "Poisson distribution", lambda = poisson$estimate)
# 
# 
# #poisson modeldistribution
# poisson <- MASS::fitdistr(iv_2005_mod$num_victim_5yr_winz_1, "Poisson")
# qqp(iv_2005_mod$num_victim_5yr_winz_1, "pois", main = "Poisson distribution", lambda = poisson$estimate)
# 

# qqp(iv_2005_mod$num_victim_5yr_winz+1, "pois", main = "Winzorized Poisson distribution", lambda = poisson$estimate)


# lattice::qqmath(m1_sample, id=0.05)
```

# Three-level logistic regression (item response method)

Talking with Dr. Bierman, he suggested that a good alternative to normal/poisson methods would be a three-level multilevel logistic regression, with the constituent count variables as the level-1 factors. The strength of this method is that it essentially treats the participant-level (level 2) as a latent, normally distributed(?) propensity to score on the outcome variables.

It firstly requires us to make the dataset extra long, with the each of the DV's constituent binary variables getting a row for *each* participant. For example, if we are counting across seven variables for 10 partcipants, our long dataset would have 70 rows.

```{r extra long data for three level}
library(tidyr)

# The arguments to gather():
# - data: Data object
# - key: Name of new key column (made from names of data columns)
# - value: Name of new value column
# - ...: Names of source columns that contain values
# - factor_key: Treat the new key column as a factor (instead of character vector)
iv_long <- gather(iv_2005_mod, measure, victimized, cartheft_5yrs:assault_5yrs, factor_key=TRUE)

iv_long$victimized <- as.factor(iv_long$victimized)

nrow(iv_long)

```

Three-level logistic regression Level 1: measure level Level 2: individual Level 3: country

Yes/no for six items Intercept would be a latent measure of propensity to say yes/no

Individual characteristics predicts the individual intercept - treat it as though it has six indicators

Like an item-response model

Can also do it in Mplus - wsmv estimator for yes/nos

```{r extra long model}
m_long <- lme4::glmer(victimized ~ (1 | pnum) + (1 | country), data = iv_long, family = binomial)
# , control = glmerControl(optimizer = "bobyqa")
# ,nAGQ = 10

summary(m_long)
```

# Other approaches

## GEE: [Generalized linear models for dependent data](https://cehs-research.github.io/eBook_multilevel/gee-count-outcome-epilepsy.html)

@snijders2011**, on Generalized Estimating Equations (GEE) (p. 198)**

-   assumes linear or generalized linear model for the *expected values* of dependent variable in a multilevel data structure, conditional on the explanatory variables

-   No assumptions made with respect to the variances and correlations, except that these are independent between highest-level units

-   linear model parameters estimated through a working model

-   standard errors estimated through sandwich estimator

-   comparison between GEE and HLM discussed in Gardiner et al. 2009

-   LARGE SAMPLE METHOD - without large sample, need a tailored small-sample version with a differently defined sandwich estimators

## Sandwich estimators

**Snijders & Bosker, on sandwich estimators (p. 173)**

> sandwich estimators for standard errors, also called cluster-robust standard errors, make no assumptions about the distributional shape of the random coefficients. Verbeke and Lesaffre (1997) and Yan and Bentler (2002) proposed sandwich estimators to provide standard errors applicable for nonnormalily distributed random effects... However, sandwich estimators do require large enough numbers of units at the highest level; see Verbeke and Lesaffre 997), Maas and Hox (2004), and hthe further discussion in 12.2 \*

[\<https://www.r-econometrics.com/methods/hcrobusterrors/\>https://stackoverflow.com/questions/48479984/r-mixed-model-with-heteroscedastic-data-only-lm-function-works](https://www.r-econometrics.com/methods/hcrobusterrors/){.uri}

> The sandwich package is not limited to lm/glm only but it is in principle object-oriented, see vignette("sandwich-OOP", package = "sandwich") (also published as <doi:10.18637/jss.v016.i09>.

> <!--# There are suitable methods for a wide variety of packages/models but not for nlme or lme4. The reason is that it's not so obvious for which mixed-effects models the usual sandwich trick actually works. (Disclaimer: But I'm no expert in mixed-effects modeling.)  -->

> However, for lme4 there is a relatively new package called merDeriv (<https://CRAN.R-project.org/package=merDeriv>) that supplies estfun and bread methods so that sandwich covariances can be computed for lmer output etc. There is also a working paper associated with that package: <https://arxiv.org/abs/1612.04911>

## Bootstrapping

[**lmeresampler**](http://aloy.github.io/lmeresampler/): provides an easy way to bootstrap nested linear-mixed effects models using either the parametric, residual, cases, CGR (semi-parametric), or random effects block (REB) bootstrap fit using either lme4 or nlme. The output from lmeresampler is an lmeresamp

**bootMer**

## Other other methods

**Match Poisson Regresssion (GLM)**

**Semi-parametric multilevel modeling** (Tihomir Asparouhov)

<!-- -   REML, by accounting for the loss in degrees of freedom from estimating the fixed effects, provides an unbiased estimate of variance components, while ML estimators for variance components are biased under assumptions of normality, since they use estimated fixed effects rather than the true values. -->

**Penalized quasilikelihood (PQL)**

flexible technique that can deal with non-normal data, unbalanced design, and crossed random effects. However, it produces biased estimates if your response variable fits a discrete count distribution, like Poisson or binomial, and the mean is less than 5 - or if your response variable is binary.

what if the mean of your response variable is less than 5, or you have a binary response variable, and you can't use PQL? Here you're going to have to make another decision, because there are two alternatives you can use: the **Laplace approximation** and **Markov chain Monte Carlo algorithms (MCMC)**. The Laplace approximation can handle up to 3 random effects. Any more than that, and you'll have to use MCMC, which is a Bayesian method that can be somewhat confusing.

<!--# https://www.juliapilowsky.com/2018/10/19/a-practical-guide-to-mixed-models-in-r/ -->

**laplace** looks like follows

    glmer(repeatgr ~ Minority + ses + ses * Minority + (1 | schoolNR),
        data = bdf, family = binomial(link = "logit"))

There is one more consideration, though, when using this method. It becomes inaccurate when used on overdispersed data - that is, when the combined residuals are much larger than the residual degrees of freedom.

**MCMCglmm**, which can not only handle many random effects, but provides confidence intervals for the random effects, MCM is a Bayesian statistical method. All models make assumptions about the distribution of the variance in your data, but in a Bayesian method these assumptions are explicit, and we need to specify these assumed distributions. In Bayesian statistics, we call these priors.

Just keep in mind that one **R structure** needs to be specified for each fixed effect and one **G structure** needs to be specified for each random effect. Also remember my caution about the lognormal distribution: these priors may not play nicely with data modeled with a log link, so do some research on what priors to use for data on a log scale.

Looks like this:

    prior <- list(R = list(V = 1, n = 0, fix = 1), G = list(G1 = list(V = 1, n = 1),
        G2 = list(V = 1, n = 1), G3 = list(V = 1, n = 1), G4 = list(V = 1, n = 1),
        G5 = list(V = 1, n = 1)))
    set.seed(45)
    MCMC <- MCMCglmm(fixed = profit ~ 1, random = ~year + farmer + place + gen + district, data = farmers, family = "categorical", prior = prior, verbose = FALSE)

<!-- **Correlated random effects** -->

<!-- @mcneish2017 -->

<!-- pwcorr rint resids -->

<!-- -   graph of correlation between within-level residuals and random effects -->

<!-- twoway (scatter resids rint) (lfit resids rint) -->

<!-- from <https://notstatschat.rbind.io/2019/04/19/progress-on-linear-mixed-models-for-surveys/> -->

<!-- ```{r heteroscedasticity, warning=F, message=FALSE, echo=FALSE, fig.show="hold", out.width="50%"} -->

<!-- # plot(m1_sample, resid(., type = "pearson") ~ fitted(.), abline = 0, main="Heteroscedasticity check - fitted values vs. residuals") -->

<!-- #  -->

<!-- # plot(m1_winz, resid(., type = "pearson") ~ fitted(.), abline = 0, main="Heteroscedasticity (Winzorized)") -->

<!-- ``` -->

<!-- > In R the function coeftest from the lmtest package can be used in combination with the function vcovHC from the sandwich package to obtain heteroskedasticity robust standard errors and their corresponding t values. -->

@mcneish2017

> [Design-based] methods estimate a general or generalized linear model and provide standard errors that are robust to misspecifications of the covariance matrix of the outcome variable. Common methods in this class include so-called heteroscedasticity-corrected covariance matrix estimators or sandwich estimators (Huber, 1967; White, 1980), and generalized estimating equations (GEEs; Liang and Zeger, 1986) DBMs are often employed when the marginal response is of interest rather than individual responses or how responses vary across clustering units.

# Limitations

The most notable weakness is that this is data from WEIRD countries. Other, less WEIRD countries (e.g., phillipines, nigeria, india) are better represented throughout sweeps 2 to 4

Unaccounted third variables (e.g., corruption)

Small clusters, limited representativeness.

# References
